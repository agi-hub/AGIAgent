#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Copyright (c) 2025 AGI Agent Research Group.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

"""
Skillæ•´ç†è„šæœ¬
åˆå¹¶ç›¸ä¼¼skillï¼Œæ¸…ç†æ— ç”¨skillï¼Œè·¨skillæ•´åˆ
"""

import os
import re
import argparse
import logging
import yaml
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import DBSCAN
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

from src.config_loader import (
    load_config, get_api_key, get_api_base, get_model,
    get_gui_default_data_directory
)
from src.tools.print_system import print_current, print_error, print_system
from .skill_tools import SkillTools


class SkillManager:
    """Skillæ•´ç†ç®¡ç†å™¨"""
    
    def __init__(self, root_dir: Optional[str] = None, config_file: str = "config/config.txt"):
        """
        åˆå§‹åŒ–Skillç®¡ç†å™¨
        
        Args:
            root_dir: æ ¹ç›®å½•ï¼ˆå¦‚æœæŒ‡å®šï¼Œè¦†ç›–configä¸­çš„è®¾ç½®ï¼‰
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.config = load_config(config_file)
        
        # ç¡®å®šæ ¹ç›®å½•
        if root_dir:
            self.root_dir = os.path.abspath(root_dir)
        else:
            data_dir = get_gui_default_data_directory(config_file)
            if data_dir:
                self.root_dir = data_dir
            else:
                project_root = self._find_project_root()
                self.root_dir = os.path.join(project_root, "data") if project_root else "data"
        
        # åˆå§‹åŒ–skillå·¥å…·ï¼ˆloggeréœ€è¦ç”¨åˆ°ï¼‰
        self.skill_tools = SkillTools(workspace_root=self.root_dir)
        
        # è®¾ç½®æ—¥å¿—ï¼ˆéœ€è¦åœ¨LLMå®¢æˆ·ç«¯åˆå§‹åŒ–ä¹‹å‰ï¼Œå› ä¸ºå¼‚å¸¸å¤„ç†ä¼šç”¨åˆ°loggerï¼‰
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.api_key = get_api_key(config_file)
        self.api_base = get_api_base(config_file)
        self.model = get_model(config_file)
        
        self.llm_client = None
        self.is_claude = False
        
        if self.api_key and self.model:
            # å‚è€ƒtask_reflection.pyçš„é€»è¾‘ï¼šå¦‚æœæ¨¡å‹ååŒ…å«claudeæˆ–api_baseåŒ…å«anthropicï¼Œä½¿ç”¨Anthropic SDK
            if 'claude' in self.model.lower() or 'anthropic' in str(self.api_base).lower():
                if ANTHROPIC_AVAILABLE:
                    try:
                        # å¯¹äºminimaxå’ŒGLMç­‰ä½¿ç”¨Anthropicå…¼å®¹APIçš„æœåŠ¡ï¼Œéœ€è¦ä¼ å…¥base_url
                        if 'bigmodel.cn' in str(self.api_base).lower() or 'minimaxi.com' in str(self.api_base).lower():
                            self.llm_client = anthropic.Anthropic(api_key=self.api_key, base_url=self.api_base)
                        else:
                            self.llm_client = anthropic.Anthropic(api_key=self.api_key)
                        self.is_claude = True
                    except Exception as e:
                        self.logger.warning(f"Failed to initialize Anthropic client: {e}")
                        self.llm_client = None
                        self.is_claude = False
                else:
                    self.logger.warning("Anthropic SDK not available, cannot initialize LLM client")
            else:
                # å¯¹äºéAnthropicæ¨¡å‹ï¼Œä½¿ç”¨OpenAIå…¼å®¹å®¢æˆ·ç«¯
                if OPENAI_AVAILABLE:
                    try:
                        self.llm_client = OpenAI(api_key=self.api_key, base_url=self.api_base)
                        self.is_claude = False
                    except Exception as e:
                        self.logger.warning(f"Failed to initialize OpenAI-compatible client: {e}")
                else:
                    self.logger.warning("OpenAI SDK not available, cannot initialize LLM client")
        else:
            missing = []
            if not self.api_key:
                missing.append("api_key")
            if not self.model:
                missing.append("model")
            self.logger.warning(f"Missing required configuration: {', '.join(missing)}, cannot initialize LLM client")
        
        # ç›¸ä¼¼åº¦é˜ˆå€¼
        self.similarity_threshold = 0.7
    
    def _find_project_root(self) -> Optional[str]:
        """æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•"""
        current = Path(__file__).resolve()
        for _ in range(10):
            config_dir = current / "config"
            if config_dir.exists() and config_dir.is_dir():
                return str(current)
            if current == current.parent:
                break
            current = current.parent
        return None
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('skill_manager')
        logger.setLevel(logging.INFO)
        
        if self.skill_tools.experience_dir:
            log_dir = os.path.join(self.skill_tools.experience_dir, "logs")
            os.makedirs(log_dir, exist_ok=True)
            
            log_file = os.path.join(log_dir, f"skill_manager_{datetime.now().strftime('%Y%m%d')}.log")
            
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)
            
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)
        
        return logger
    
    def _load_all_skills(self) -> List[Tuple[str, Dict[str, Any]]]:
        """
        åŠ è½½æ‰€æœ‰skillæ–‡ä»¶
        
        Returns:
            [(æ–‡ä»¶è·¯å¾„, skillæ•°æ®), ...] åˆ—è¡¨
        """
        if not self.skill_tools.experience_dir:
            return []
        
        skills = []
        for filename in os.listdir(self.skill_tools.experience_dir):
            if filename.startswith('skill_') and filename.endswith('.md'):
                file_path = os.path.join(self.skill_tools.experience_dir, filename)
                try:
                    skill_data = self.skill_tools._load_skill_file(file_path)
                    if skill_data:
                        skills.append((file_path, skill_data))
                except Exception as e:
                    self.logger.error(f"Error loading skill file {file_path}: {e}")
                    print_error(f"Error loading skill file {file_path}: {e}, skipping...")
        
        return skills
    
    def _calculate_similarity_matrix(self, skills: List[Tuple[str, Dict[str, Any]]]) -> Tuple[List[List[float]], List[str]]:
        """
        è®¡ç®—skillä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        
        Args:
            skills: skillåˆ—è¡¨
            
        Returns:
            (ç›¸ä¼¼åº¦çŸ©é˜µ, skill_idåˆ—è¡¨)
        """
        if not SKLEARN_AVAILABLE:
            return [], []
        
        texts = []
        skill_ids = []
        
        for file_path, skill_data in skills:
            front_matter = skill_data['front_matter']
            content = skill_data['content']
            
            title = front_matter.get('title', '')
            usage_conditions = front_matter.get('usage_conditions', '')
            combined_text = f"{title} {usage_conditions} {content}"
            
            texts.append(combined_text)
            skill_ids.append(str(front_matter.get('skill_id', '')))
        
        if not texts:
            return [], []
        
        try:
            vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
            tfidf_matrix = vectorizer.fit_transform(texts)
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            return similarity_matrix.tolist(), skill_ids
        except Exception as e:
            self.logger.error(f"Error calculating similarity matrix: {e}")
            return [], []
    
    def _merge_similar_skills(self, skills: List[Tuple[str, Dict[str, Any]]]) -> int:
        """
        åˆå¹¶ç›¸ä¼¼åº¦é«˜çš„skill
        
        Args:
            skills: skillåˆ—è¡¨
            
        Returns:
            åˆå¹¶çš„skillæ•°é‡
        """
        if not SKLEARN_AVAILABLE:
            self.logger.warning("scikit-learn not available, skipping similarity merge")
            return 0
        
        if len(skills) < 2:
            return 0
        
        similarity_matrix, skill_ids = self._calculate_similarity_matrix(skills)
        if not similarity_matrix:
            return 0
        
        merged_count = 0
        processed = set()
        
        # åˆ›å»ºskill_idåˆ°ç´¢å¼•çš„æ˜ å°„
        skill_id_to_idx = {sid: idx for idx, sid in enumerate(skill_ids)}
        idx_to_skill = {idx: skill for idx, skill in enumerate(skills)}
        
        for i in range(len(skills)):
            if i in processed:
                continue
            
            # æŸ¥æ‰¾ç›¸ä¼¼åº¦é«˜çš„skill
            similar_indices = []
            for j in range(i + 1, len(skills)):
                if j in processed:
                    continue
                
                similarity = similarity_matrix[i][j]
                if similarity > self.similarity_threshold:
                    similar_indices.append(j)
            
            if not similar_indices:
                continue
            
            # åˆå¹¶skill
            main_skill = skills[i]
            main_front_matter = main_skill[1]['front_matter']
            main_content = main_skill[1]['content']
            main_quality = main_front_matter.get('quality_index', 0.5)
            
            # æ‰¾åˆ°è´¨é‡æŒ‡æ•°æœ€é«˜çš„ä½œä¸ºä¸»skill
            for idx in similar_indices:
                other_skill = skills[idx]
                other_front_matter = other_skill[1]['front_matter']
                other_quality = other_front_matter.get('quality_index', 0.5)
                
                if other_quality > main_quality:
                    main_skill = other_skill
                    main_front_matter = other_front_matter
                    main_content = other_skill[1]['content']
                    main_quality = other_quality
            
            # åˆå¹¶å†…å®¹
            merged_content = main_content
            merged_task_dirs = list(main_front_matter.get('task_directories', []))
            merged_fetch_count = main_front_matter.get('fetch_count', 0)
            
            for idx in similar_indices:
                other_skill = skills[idx]
                other_front_matter = other_skill[1]['front_matter']
                other_content = other_skill[1]['content']
                
                # åˆå¹¶å†…å®¹
                if other_content not in merged_content:
                    merged_content += f"\n\n---\n\n{other_content}"
                
                # åˆå¹¶task_directories
                other_dirs = other_front_matter.get('task_directories', [])
                for dir_name in other_dirs:
                    if dir_name not in merged_task_dirs:
                        merged_task_dirs.append(dir_name)
                
                # åˆå¹¶fetch_count
                merged_fetch_count += other_front_matter.get('fetch_count', 0)
                
                # åˆ é™¤å…¶ä»–skillï¼ˆç§»åŠ¨åˆ°legacyï¼‰
                other_skill_id = str(other_front_matter.get('skill_id', ''))
                result = self.skill_tools.delete_skill(other_skill_id)
                if result.get('status') == 'success':
                    merged_count += 1
                    processed.add(idx)
            
            # æ›´æ–°ä¸»skill
            main_front_matter['task_directories'] = merged_task_dirs
            main_front_matter['fetch_count'] = merged_fetch_count
            main_front_matter['updated_at'] = datetime.now().isoformat()
            
            # é‡æ–°è®¡ç®—è´¨é‡æŒ‡æ•°ï¼ˆåŠ æƒå¹³å‡ï¼‰
            if similar_indices:
                qualities = [main_quality]
                for idx in similar_indices:
                    other_quality = skills[idx][1]['front_matter'].get('quality_index', 0.5)
                    qualities.append(other_quality)
                avg_quality = sum(qualities) / len(qualities)
                main_front_matter['quality_index'] = round(avg_quality, 3)
            
            self.skill_tools._save_skill_file(main_skill[0], main_front_matter, merged_content)
            processed.add(i)
        
        return merged_count
    
    def _cluster_by_usage_conditions(self, skills: List[Tuple[str, Dict[str, Any]]]) -> Dict[int, List[int]]:
        """
        åŸºäºusage_conditionså¯¹skillè¿›è¡Œèšç±»
        
        Args:
            skills: skillåˆ—è¡¨
            
        Returns:
            {èšç±»ID: [skillç´¢å¼•åˆ—è¡¨], ...} å­—å…¸
        """
        clusters = {}
        cluster_id = 0
        usage_to_indices = {}
        
        # æŒ‰usage_conditionsåˆ†ç»„
        for idx, (file_path, skill_data) in enumerate(skills):
            front_matter = skill_data['front_matter']
            usage = front_matter.get('usage_conditions', '').strip()
            
            # æå–å…³é”®è¯ï¼ˆå»é™¤å¸¸è§è¯ï¼‰
            if usage:
                # ç®€å•æå–å…³é”®å·¥å…·æˆ–æ“ä½œ
                key_terms = []
                if 'custom_command' in usage.lower():
                    key_terms.append('custom_command')
                if 'game' in usage.lower():
                    key_terms.append('game')
                if 'type=' in usage.lower():
                    # æå–typeçš„å€¼
                    import re
                    type_match = re.search(r"type=['\"]?(\w+)['\"]?", usage, re.IGNORECASE)
                    if type_match:
                        key_terms.append(f"type_{type_match.group(1)}")
                
                # åˆ›å»ºèšç±»é”®
                cluster_key = '_'.join(sorted(key_terms)) if key_terms else usage[:50]
            else:
                cluster_key = 'unknown'
            
            if cluster_key not in usage_to_indices:
                usage_to_indices[cluster_key] = []
            usage_to_indices[cluster_key].append(idx)
        
        # åªä¿ç•™åŒ…å«è‡³å°‘2ä¸ªskillçš„èšç±»
        for cluster_key, indices in usage_to_indices.items():
            if len(indices) >= 2:
                clusters[cluster_id] = indices
                cluster_id += 1
        
        return clusters
    
    def _cluster_skills_with_dbscan(self, skills: List[Tuple[str, Dict[str, Any]]]) -> Dict[int, List[int]]:
        """
        ä½¿ç”¨DBSCANå¯¹skillè¿›è¡Œèšç±»
        
        Args:
            skills: skillåˆ—è¡¨
            
        Returns:
            {èšç±»ID: [skillç´¢å¼•åˆ—è¡¨], ...} å­—å…¸
        """
        if not SKLEARN_AVAILABLE:
            return {}
        
        if len(skills) < 2:
            return {}
        
        similarity_matrix, skill_ids = self._calculate_similarity_matrix(skills)
        if not similarity_matrix:
            return {}
        
        try:
            # è½¬æ¢ä¸ºè·ç¦»çŸ©é˜µï¼ˆ1 - ç›¸ä¼¼åº¦ï¼‰
            import numpy as np
            similarity_array = np.array(similarity_matrix)
            
            # ç¡®ä¿ç›¸ä¼¼åº¦å€¼åœ¨[0, 1]èŒƒå›´å†…
            similarity_array = np.clip(similarity_array, 0.0, 1.0)
            
            # è½¬æ¢ä¸ºè·ç¦»çŸ©é˜µï¼ˆ1 - ç›¸ä¼¼åº¦ï¼‰ï¼Œç¡®ä¿è·ç¦»å€¼éè´Ÿ
            distance_matrix = 1.0 - similarity_array
            distance_matrix = np.clip(distance_matrix, 0.0, 1.0)
            
            # DBSCANèšç±» - å°è¯•å¤šä¸ªepså€¼
            # é¦–å…ˆå°è¯•æ ‡å‡†å‚æ•°
            dbscan = DBSCAN(eps=0.5, min_samples=2, metric='precomputed')
            labels = dbscan.fit_predict(distance_matrix)
            
            # ç»Ÿè®¡èšç±»ç»“æœ
            unique_labels = set(labels)
            noise_count = list(labels).count(-1)
            cluster_count = len(unique_labels) - (1 if -1 in unique_labels else 0)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°èšç±»ï¼Œå°è¯•æ›´å®½æ¾çš„å‚æ•°
            if cluster_count == 0 and len(skills) >= 2:
                dbscan = DBSCAN(eps=0.6, min_samples=2, metric='precomputed')
                labels = dbscan.fit_predict(distance_matrix)
                unique_labels = set(labels)
                noise_count = list(labels).count(-1)
                cluster_count = len(unique_labels) - (1 if -1 in unique_labels else 0)
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰èšç±»ï¼Œå°è¯•åŸºäºç›¸ä¼¼åº¦çš„ç®€å•èšç±»ï¼ˆé€æ­¥é™ä½é˜ˆå€¼ï¼‰
            if cluster_count == 0 and len(skills) >= 2:
                # å°è¯•å¤šä¸ªé˜ˆå€¼ï¼Œä»é«˜åˆ°ä½
                thresholds = [0.3, 0.2, 0.15, 0.1]
                for threshold in thresholds:
                    clusters = {}
                    cluster_id = 0
                    assigned = set()
                    
                    for i in range(len(skills)):
                        if i in assigned:
                            continue
                        
                        # æ‰¾åˆ°ä¸å½“å‰skillç›¸ä¼¼çš„skill
                        similar_indices = [i]
                        for j in range(i + 1, len(skills)):
                            if j in assigned:
                                continue
                            if similarity_array[i][j] > threshold:
                                similar_indices.append(j)
                        
                        if len(similar_indices) >= 2:
                            clusters[cluster_id] = similar_indices
                            assigned.update(similar_indices)
                            cluster_id += 1
                    
                    if clusters:
                        print_current(f"âœ… æ‰¾åˆ° {len(clusters)} ä¸ªskillèšç±» (ç›¸ä¼¼åº¦é˜ˆå€¼: {threshold})")
                        return clusters
                
                # å¦‚æœæ‰€æœ‰é˜ˆå€¼éƒ½å¤±è´¥ï¼Œå°è¯•åŸºäºusage_conditionsçš„èšç±»
                clusters = self._cluster_by_usage_conditions(skills)
                if clusters:
                    print_current(f"âœ… åŸºäºä½¿ç”¨æ¡ä»¶æ‰¾åˆ° {len(clusters)} ä¸ªskillèšç±»")
                    return clusters
            
            # ç»„ç»‡DBSCANèšç±»ç»“æœ
            clusters = {}
            for idx, label in enumerate(labels):
                if label != -1:  # -1è¡¨ç¤ºå™ªå£°ç‚¹
                    if label not in clusters:
                        clusters[label] = []
                    clusters[label].append(idx)
            
            return clusters
        except Exception as e:
            self.logger.error(f"Error in DBSCAN clustering: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {}
    
    def _call_llm_for_merge_decision(self, skill_group: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è°ƒç”¨LLMå†³å®šæ˜¯å¦åˆå¹¶skillç»„
        
        Args:
            skill_group: skillç»„åˆ—è¡¨
            
        Returns:
            LLMå†³ç­–ç»“æœï¼ŒåŒ…å«æ˜¯å¦åˆå¹¶å’Œåˆå¹¶åçš„å†…å®¹
        """
        if not self.llm_client:
            return {
                'should_merge': False,
                'reason': 'LLM client not available'
            }
        
        if len(skill_group) < 2:
            return {
                'should_merge': False,
                'reason': 'Not enough skills to merge'
            }
        
        # æ„å»ºæç¤º - æ ¹æ®éœ€æ±‚æ–‡æ¡£ï¼Œå¼ºè°ƒè·¨ä»»åŠ¡ç»éªŒæ€»ç»“
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªskillæ•´åˆä¸“å®¶ï¼Œä¸“é—¨è´Ÿè´£å°†å¤šä¸ªä»»åŠ¡çš„skillæ•´åˆæˆæ›´é«˜çº§çš„ç»¼åˆskillã€‚

**é‡è¦è¯´æ˜ï¼š**
è¿™äº›skillå·²ç»è¢«ç›¸ä¼¼åº¦ç®—æ³•è¯†åˆ«ä¸ºç›¸å…³ä»»åŠ¡ï¼Œå®ƒä»¬æ¥è‡ªç›¸ä¼¼çš„ä»»åŠ¡ç±»å‹æˆ–æ‰§è¡Œåœºæ™¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æå®ƒä»¬ï¼Œå¹¶å†³å®šæ˜¯å¦åº”è¯¥æ•´åˆæˆä¸€ä¸ªæ›´é«˜çº§çš„ç»¼åˆskillã€‚

**æ ¸å¿ƒç›®æ ‡ï¼š**
è·¨ä»»åŠ¡skillæ•´åˆçš„ç›®çš„æ˜¯å½¢æˆæ›´é«˜çº§çš„æ–°skillï¼ˆå¤šä»»åŠ¡ç»¼åˆå‡ºæ¥çš„skillï¼‰ï¼Œç‰¹åˆ«æ˜¯è¦æ€»ç»“å‡ºï¼š
1. **æˆåŠŸç»éªŒ**ï¼šå“ªäº›ç­–ç•¥å’Œæ–¹æ³•å¯¼è‡´äº†ä»»åŠ¡æˆåŠŸå®Œæˆ
2. **å¤±è´¥æ•™è®­**ï¼šå“ªäº›åšæ³•å¯¼è‡´äº†ä»»åŠ¡å¤±è´¥æˆ–æœªå®Œæˆ
3. **æˆåŠŸè·¯å¾„**ï¼šå¦‚ä½•ä»å¤±è´¥èµ°å‘æˆåŠŸï¼Œæœ€ç»ˆæˆåŠŸçš„æ ¸å¿ƒæ–¹æ³•å’Œå…³é”®æ­¥éª¤
4. **ä»»åŠ¡è§„å¾‹**ï¼šæ€»ç»“ä»»åŠ¡ç±»å‹ã€æ‰§è¡Œè§„å¾‹å’Œå…³é”®æˆåŠŸå› ç´ 

**é‡è¦è¦æ±‚ï¼š**
- å¿…é¡»ä½¿ç”¨ä¸­æ–‡è¾“å‡º
- **å¦‚æœè¿™äº›skillæ¥è‡ªç›¸ä¼¼çš„ä»»åŠ¡ç±»å‹ï¼Œåº”è¯¥å€¾å‘äºåˆå¹¶**ï¼Œé™¤éå®ƒä»¬ç¡®å®æ— æ³•æ•´åˆ
- å¿…é¡»å¯¹æ¯”æˆåŠŸå’Œå¤±è´¥çš„æ¡ˆä¾‹ï¼Œæ·±å…¥åˆ†ææˆåŠŸä¸å¤±è´¥çš„æ ¹æœ¬åŸå› 
- å¯¹äºå¤šæ¬¡å°è¯•çš„ä»»åŠ¡ï¼Œå¿…é¡»æ€»ç»“å‡ºæœ€çŸ­æˆåŠŸè·¯å¾„å’Œå…³é”®æˆåŠŸå› ç´ 
- æ•´åˆåçš„skillåº”è¯¥æ¯”å•ä¸ªskillæ›´æœ‰ä»·å€¼ï¼Œèƒ½å¤ŸæŒ‡å¯¼æœªæ¥ç±»ä¼¼ä»»åŠ¡çš„æ‰§è¡Œ
- é‡ç‚¹å…³æ³¨ä»»åŠ¡æ‰§è¡Œçš„å¤±è´¥/æˆåŠŸæ€»ç»“ï¼Œæç‚¼å‡ºå¯å¤ç”¨çš„ç»éªŒå’Œæ•™è®­
- **å¿…é¡»æ˜ç¡®è¯´æ˜åˆå¹¶çš„ç†ç”±ï¼Œå³ä½¿å†³å®šä¸åˆå¹¶ä¹Ÿè¦ç»™å‡ºè¯¦ç»†åŸå› **

**è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰ç…§æ­¤æ ¼å¼è¾“å‡ºï¼‰ï¼š**
MERGE: yes
REASON: åˆå¹¶ç†ç”±ï¼ˆè¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›skillå¯ä»¥æ•´åˆï¼Œæ•´åˆåçš„ä»·å€¼ã€‚å¦‚æœä¸åˆå¹¶ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆä¸åˆå¹¶ï¼‰
TITLE: æ–°skillæ ‡é¢˜ï¼ˆç®€æ´æ˜ç¡®ï¼Œä½“ç°ç»¼åˆç»éªŒã€‚å¦‚æœä¸åˆå¹¶ï¼Œå¯ä»¥ç•™ç©ºï¼‰
USAGE_CONDITIONS: æ–°skillä½¿ç”¨æ¡ä»¶ï¼ˆå…·ä½“æ˜ç¡®ï¼Œè¯´æ˜ä½•æ—¶ä½¿ç”¨ã€‚å¦‚æœä¸åˆå¹¶ï¼Œå¯ä»¥ç•™ç©ºï¼‰
CONTENT: åˆå¹¶åçš„è¯¦ç»†å†…å®¹ï¼ˆå¿…é¡»åŒ…å«ï¼š1. ä»»åŠ¡ç±»å‹å’Œæ‰§è¡Œè§„å¾‹æ€»ç»“ï¼›2. æˆåŠŸç»éªŒï¼›3. å¤±è´¥æ•™è®­ï¼›4. æˆåŠŸç­–ç•¥å’Œå…³é”®æ–¹æ³•ï¼›5. æœ€çŸ­æˆåŠŸè·¯å¾„ï¼›6. ç”¨æˆ·åå¥½ã€‚å¦‚æœä¸åˆå¹¶ï¼Œå¯ä»¥ç•™ç©ºï¼‰"""

        skill_descriptions = []
        for i, skill in enumerate(skill_group, 1):
            front_matter = skill['front_matter']
            # ä¼ é€’å®Œæ•´çš„skillå†…å®¹ï¼Œè€Œä¸æ˜¯åªä¼ é€’å‰500ä¸ªå­—ç¬¦
            full_content = skill['content']
            
            # æ£€æŸ¥skillæ˜¯å¦æ¶‰åŠæˆåŠŸæˆ–å¤±è´¥
            title = front_matter.get('title', '')
            is_success = 'æˆåŠŸ' in title or 'å®Œæˆ' in title or 'è·èƒœ' in title or 'æˆ˜èƒœ' in title
            is_failure = 'å¤±è´¥' in title or 'æœªå®Œæˆ' in title or 'è¾“' in title or 'æƒœè´¥' in title
            
            skill_descriptions.append(f"""
Skill {i}:
- ID: {front_matter.get('skill_id')}
- Title: {front_matter.get('title')}
- Usage Conditions: {front_matter.get('usage_conditions')}
- Quality Index: {front_matter.get('quality_index')}
- ä»»åŠ¡ç»“æœ: {'æˆåŠŸ' if is_success else 'å¤±è´¥' if is_failure else 'æœªçŸ¥'}
- å®Œæ•´å†…å®¹:
{full_content}
""")
        
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹{len(skill_group)}ä¸ªç›¸å…³çš„skillï¼Œå®ƒä»¬æ¥è‡ªä¸åŒçš„ä»»åŠ¡æ‰§è¡Œã€‚

**é‡è¦æç¤ºï¼š**è¿™äº›skillå·²ç»è¢«ç›¸ä¼¼åº¦ç®—æ³•è¯†åˆ«ä¸ºç›¸å…³ä»»åŠ¡ï¼Œå®ƒä»¬å¾ˆå¯èƒ½æ¥è‡ªç›¸ä¼¼çš„ä»»åŠ¡ç±»å‹æˆ–æ‰§è¡Œåœºæ™¯ã€‚è¯·ä»”ç»†åˆ†æå®ƒä»¬ï¼Œ**å¦‚æœå®ƒä»¬ç¡®å®ç›¸å…³ï¼Œåº”è¯¥å€¾å‘äºåˆå¹¶**ã€‚

{''.join(skill_descriptions)}

**åˆ†æè¦æ±‚ï¼š**
1. **é¦–å…ˆåˆ¤æ–­**ï¼šè¿™äº›skillæ˜¯å¦æ¥è‡ªç›¸ä¼¼çš„ä»»åŠ¡ç±»å‹æˆ–æ‰§è¡Œåœºæ™¯ï¼Ÿå¦‚æœç›¸ä¼¼ï¼Œåº”è¯¥åˆå¹¶ã€‚
2. **é‡ç‚¹å¯¹æ¯”æˆåŠŸå’Œå¤±è´¥çš„æ¡ˆä¾‹**ï¼š
   - åˆ†ææˆåŠŸæ¡ˆä¾‹ä¸­çš„å…³é”®ç­–ç•¥ã€æ–¹æ³•å’Œæ“ä½œ
   - åˆ†æå¤±è´¥æ¡ˆä¾‹ä¸­çš„é”™è¯¯åšæ³•ã€å¤±è´¥åŸå› å’Œå…³é”®é—®é¢˜ç‚¹
   - æ€»ç»“å‡ºä»»åŠ¡æˆåŠŸå®Œæˆçš„æ ¸å¿ƒæ–¹æ³•å’Œå¿…è¦æ¡ä»¶
   - æ€»ç»“å‡ºå¯¼è‡´ä»»åŠ¡å¤±è´¥çš„ä¸»è¦åŸå› å’Œéœ€è¦é¿å…çš„é™·é˜±
3. å¦‚æœæ¶‰åŠå¤šæ¬¡è¿­ä»£çš„ä»»åŠ¡ï¼Œå¿…é¡»æ€»ç»“ï¼š
   - å¤±è´¥çš„ä¸»è¦åŸå› ï¼ˆå…³é”®ç‚¹ï¼‰
   - æœ€ç»ˆæˆåŠŸçš„ç­–ç•¥ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
   - æœ€çŸ­æˆåŠŸè·¯å¾„å’Œå…³é”®æ­¥éª¤
4. æ•´åˆåçš„skillåº”è¯¥èƒ½å¤ŸæŒ‡å¯¼æœªæ¥æ‰§è¡Œç±»ä¼¼ä»»åŠ¡æ—¶ï¼š
   - å¦‚ä½•é¿å…å¤±è´¥
   - å¦‚ä½•æˆåŠŸå®Œæˆä»»åŠ¡
   - é‡‡ç”¨å“ªäº›å…³é”®ç­–ç•¥å’Œæ–¹æ³•

**è¯·ä¸¥æ ¼æŒ‰ç…§è¾“å‡ºæ ¼å¼å›ç­”ï¼š**
- å¦‚æœè¿™äº›skillæ¥è‡ªç›¸ä¼¼ä»»åŠ¡ï¼Œè¾“å‡º MERGE: yesï¼Œå¹¶æä¾›å®Œæ•´çš„æ•´åˆå†…å®¹
- å¦‚æœç¡®å®æ— æ³•æ•´åˆï¼Œè¾“å‡º MERGE: noï¼Œå¹¶åœ¨REASONä¸­è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆä¸åˆå¹¶"""
        
        try:
            print_current(f"ğŸ”„ è°ƒç”¨LLMè¿›è¡Œskillæ•´åˆå†³ç­–...")
            if self.is_claude:
                response = self.llm_client.messages.create(
                    model=self.model,
                    max_tokens=6000,  # å¢åŠ tokenæ•°é‡ä»¥æ”¯æŒå®Œæ•´å†…å®¹æ•´åˆ
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}],
                    temperature=0.7
                )
                decision_text = response.content[0].text if response.content else ""
            else:
                response = self.llm_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=6000,  # å¢åŠ tokenæ•°é‡ä»¥æ”¯æŒå®Œæ•´å†…å®¹æ•´åˆ
                    temperature=0.7
                )
                
                # æ£€æŸ¥APIå“åº”æ˜¯å¦æœ‰é”™è¯¯
                if hasattr(response, 'success') and response.success is False:
                    error_msg = getattr(response, 'msg', 'Unknown error')
                    error_code = getattr(response, 'code', 'Unknown')
                    self.logger.error(f"APIè°ƒç”¨å¤±è´¥: code={error_code}, msg={error_msg}")
                    print_error(f"âŒ APIè°ƒç”¨å¤±è´¥: {error_msg} (code: {error_code})")
                    print_error(f"   è¯·æ£€æŸ¥APIé…ç½®ã€æ¨¡å‹åç§°å’Œç«¯ç‚¹æ˜¯å¦æ­£ç¡®")
                    return {
                        'should_merge': False,
                        'reason': f'APIè°ƒç”¨å¤±è´¥: {error_msg} (code: {error_code})'
                    }
                
                decision_text = response.choices[0].message.content if response.choices else ""
            
            # æ£€æŸ¥å“åº”ç»“æ„
            if not decision_text:
                if self.is_claude:
                    self.logger.error(f"Claude API response structure: {response}")
                    print_error(f"âŒ Claude APIå“åº”ç»“æ„å¼‚å¸¸: {response}")
                else:
                    # æ£€æŸ¥æ˜¯å¦æœ‰choices
                    if not hasattr(response, 'choices') or not response.choices:
                        self.logger.error(f"OpenAI API response has no choices: {response}")
                        print_error(f"âŒ OpenAI APIå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µ")
                        print_error(f"   å“åº”å¯¹è±¡: {response}")
                        # å°è¯•è·å–é”™è¯¯ä¿¡æ¯
                        if hasattr(response, 'error'):
                            print_error(f"   é”™è¯¯ä¿¡æ¯: {response.error}")
                        return {
                            'should_merge': False,
                            'reason': 'APIå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µï¼Œå¯èƒ½æ˜¯APIè°ƒç”¨å¤±è´¥'
                        }
                    self.logger.error(f"OpenAI API response structure: {response}")
                    print_error(f"âŒ OpenAI APIå“åº”ç»“æ„å¼‚å¸¸: {response}")
            
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not decision_text or not decision_text.strip():
                self.logger.error("LLM returned empty response!")
                print_error("âŒ LLMè¿”å›ç©ºå“åº”ï¼Œè¯·æ£€æŸ¥APIé…ç½®å’Œç½‘ç»œè¿æ¥")
                return {
                    'should_merge': False,
                    'reason': 'LLMè¿”å›ç©ºå“åº”ï¼Œå¯èƒ½æ˜¯APIè°ƒç”¨å¤±è´¥æˆ–é…ç½®é—®é¢˜'
                }
            
            # è®°å½•LLMå®Œæ•´å“åº”ç”¨äºè°ƒè¯•
            print_current(f"ğŸ“ LLMå“åº” (å‰500å­—ç¬¦): {decision_text[:500]}")
            if len(decision_text) > 500:
                print_current(f"ğŸ“ LLMå“åº” (ç»§ç»­): ...{decision_text[500:1000]}")
            self.logger.info(f"LLMå®Œæ•´å“åº”: {decision_text}")
            
            # è§£æå†³ç­– - æ”¯æŒå¤šç§æ ¼å¼
            decision_upper = decision_text.upper()
            # æ£€æŸ¥æ˜¯å¦æ˜ç¡®è¯´ä¸åˆå¹¶
            should_not_merge = 'MERGE: no' in decision_upper or 'MERGE:NO' in decision_upper or 'MERGE: NO' in decision_upper
            # æ£€æŸ¥æ˜¯å¦æ˜ç¡®è¯´åˆå¹¶
            should_merge = ('MERGE: yes' in decision_upper or 'MERGE:YES' in decision_upper or 
                          'MERGE: YES' in decision_upper)
            
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„yes/noï¼Œå°è¯•ä»ä¸Šä¸‹æ–‡æ¨æ–­
            if not should_merge and not should_not_merge:
                # å¦‚æœæåˆ°äº†"åˆå¹¶"ã€"æ•´åˆ"ç­‰å…³é”®è¯ï¼Œä¸”æ²¡æœ‰æ˜ç¡®æ‹’ç»ï¼Œå€¾å‘äºåˆå¹¶
                if any(keyword in decision_text for keyword in ['åˆå¹¶', 'æ•´åˆ', 'ç»¼åˆ', 'æ•´åˆå', 'åˆå¹¶å']):
                    if not any(keyword in decision_text for keyword in ['ä¸åˆå¹¶', 'ä¸æ•´åˆ', 'æ— æ³•åˆå¹¶', 'ä¸èƒ½åˆå¹¶']):
                        should_merge = True
                        self.logger.info("Inferred merge decision from context keywords")
            
            # æå–ä¿¡æ¯ - æ”¹è¿›è§£æé€»è¾‘ï¼Œæ”¯æŒå¤šè¡Œå†…å®¹å’Œä¸­æ–‡å†’å·
            reason = ""
            # å°è¯•å¤šç§æ ¼å¼ï¼šREASON:ã€REASONï¼šã€åŸå› ï¼šç­‰
            reason_markers = ['REASON:', 'REASONï¼š', 'åŸå› :', 'åŸå› ï¼š', 'ç†ç”±:', 'ç†ç”±ï¼š']
            for marker in reason_markers:
                if marker in decision_text:
                    reason_part = decision_text.split(marker, 1)[1]
                    # æå–åˆ°ä¸‹ä¸€ä¸ªæ ‡è®°æˆ–æ®µè½ç»“æŸ
                    next_markers = ['TITLE:', 'TITLEï¼š', 'æ ‡é¢˜:', 'æ ‡é¢˜ï¼š', 'USAGE_CONDITIONS:', 'USAGE_CONDITIONSï¼š', 
                                   'ä½¿ç”¨æ¡ä»¶:', 'ä½¿ç”¨æ¡ä»¶ï¼š', 'CONTENT:', 'CONTENTï¼š', 'å†…å®¹:', 'å†…å®¹ï¼š', '\n\n']
                    for next_marker in next_markers:
                        if next_marker in reason_part:
                            reason = reason_part.split(next_marker)[0].strip()
                            break
                    if not reason:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå–ç¬¬ä¸€è¡Œæˆ–å‰200å­—ç¬¦
                        reason = reason_part.split('\n')[0].strip()[:200]
                    break
            
            if not reason and not should_merge:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°reasonæ ‡è®°ï¼Œå°è¯•ä»å†³ç­–æ–‡æœ¬ä¸­æå–
                reason = "LLMæœªæä¾›æ˜ç¡®çš„åˆå¹¶ç†ç”±"
            
            title = ""
            if 'TITLE:' in decision_text:
                title_part = decision_text.split('TITLE:')[1]
                # æå–åˆ°ä¸‹ä¸€ä¸ªæ ‡è®°æˆ–æ®µè½ç»“æŸ
                if 'USAGE_CONDITIONS:' in title_part:
                    title = title_part.split('USAGE_CONDITIONS:')[0].strip()
                elif 'CONTENT:' in title_part:
                    title = title_part.split('CONTENT:')[0].strip()
                else:
                    title = title_part.split('\n\n')[0].strip()
            
            usage_conditions = ""
            if 'USAGE_CONDITIONS:' in decision_text:
                usage_part = decision_text.split('USAGE_CONDITIONS:')[1]
                # æå–åˆ°CONTENTæ ‡è®°æˆ–æ®µè½ç»“æŸ
                if 'CONTENT:' in usage_part:
                    usage_conditions = usage_part.split('CONTENT:')[0].strip()
                else:
                    usage_conditions = usage_part.split('\n\n')[0].strip()
            
            content = ""
            if 'CONTENT:' in decision_text:
                content = decision_text.split('CONTENT:')[1].strip()
            elif should_merge and not content:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„CONTENTæ ‡è®°ï¼Œä½†å†³å®šåˆå¹¶ï¼Œå°è¯•æå–æ•´ä¸ªå†³ç­–æ–‡æœ¬ä½œä¸ºå†…å®¹
                # è·³è¿‡å‰é¢çš„æ ‡è®°éƒ¨åˆ†
                content_start = max(
                    decision_text.find('CONTENT:'),
                    decision_text.find('TITLE:'),
                    decision_text.find('USAGE_CONDITIONS:'),
                    decision_text.find('REASON:')
                )
                if content_start > 0:
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªæ ‡è®°åçš„å†…å®¹
                    last_marker = max(
                        decision_text.rfind('TITLE:'),
                        decision_text.rfind('USAGE_CONDITIONS:'),
                        decision_text.rfind('REASON:')
                    )
                    if last_marker > 0:
                        content = decision_text[last_marker:].split(':', 1)[1].strip() if ':' in decision_text[last_marker:] else decision_text[last_marker:].strip()
            
            return {
                'should_merge': should_merge,
                'reason': reason,
                'title': title,
                'usage_conditions': usage_conditions,
                'content': content
            }
        
        except Exception as e:
            error_str = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯
            if '401' in error_str or 'authentication' in error_str.lower() or 'invalid' in error_str.lower() and 'key' in error_str.lower():
                self.logger.warning(f"LLM API authentication error: {e}. Please check your API key in config file.")
                return {
                    'should_merge': False,
                    'reason': 'LLM API authentication failed. Please check your API key configuration.'
                }
            else:
                self.logger.error(f"Error calling LLM for merge decision: {e}")
                return {
                    'should_merge': False,
                    'reason': f'Error in LLM call: {str(e)}'
                }
    
    def _clean_unused_skills(self, skills: List[Tuple[str, Dict[str, Any]]]) -> int:
        """
        æ¸…ç†é•¿æœŸä¸ä½¿ç”¨çš„skill
        
        Args:
            skills: skillåˆ—è¡¨
            
        Returns:
            æ¸…ç†çš„skillæ•°é‡
        """
        cleaned_count = 0
        cutoff_date = datetime.now() - timedelta(days=30)
        
        for file_path, skill_data in skills:
            front_matter = skill_data['front_matter']
            fetch_count = front_matter.get('fetch_count', 0)
            created_at_str = front_matter.get('created_at', '')
            
            if fetch_count == 0 and created_at_str:
                try:
                    created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))
                    if created_at < cutoff_date:
                        skill_id = str(front_matter.get('skill_id', ''))
                        result = self.skill_tools.delete_skill(skill_id)
                        if result.get('status') == 'success':
                            cleaned_count += 1
                            self.logger.info(f"Cleaned unused skill: {skill_id}")
                except Exception:
                    pass
        
        return cleaned_count
    
    def run(self):
        """è¿è¡Œskillæ•´ç†æµç¨‹"""
        self.logger.info("Starting skill management process")
        print_system("Starting skill management process")
        
        # åŠ è½½æ‰€æœ‰skill
        skills = self._load_all_skills()
        
        if not skills:
            self.logger.info("No skills found")
            print_current("No skills found")
            return
        
        self.logger.info(f"Loaded {len(skills)} skills")
        print_current(f"Loaded {len(skills)} skills")
        
        # 1. åŸºç¡€åˆå¹¶ï¼ˆç›¸ä¼¼åº¦ > 0.7ï¼‰
        print_current("Step 1: Merging similar skills...")
        merged_count = self._merge_similar_skills(skills)
        self.logger.info(f"Merged {merged_count} similar skills")
        print_current(f"âœ… Merged {merged_count} similar skills")
        
        # é‡æ–°åŠ è½½skillï¼ˆå› ä¸ºå¯èƒ½æœ‰å˜åŒ–ï¼‰
        skills = self._load_all_skills()
        
        # 2. DBSCANèšç±»å’Œè·¨skillæ•´åˆ
        if SKLEARN_AVAILABLE and len(skills) >= 2:
            print_current("Step 2: Cross-skill integration...")
            
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not self.llm_client:
                print_current("âš ï¸  LLMå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡è·¨skillæ•´åˆã€‚è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®æœ‰æ•ˆçš„APIå¯†é’¥ã€‚")
                self.logger.warning("LLM client not initialized, skipping cross-skill integration step")
                integrated_count = 0
            else:
                # æ˜¾ç¤ºAPIé…ç½®ä¿¡æ¯ç”¨äºè°ƒè¯•
                print_current(f"ğŸ”§ APIé…ç½®: model={self.model}, api_base={self.api_base}, is_claude={self.is_claude}")
                self.logger.info(f"API config: model={self.model}, api_base={self.api_base}, is_claude={self.is_claude}")
                # åˆå§‹åŒ–integrated_count
                integrated_count = 0
                
                clusters = self._cluster_skills_with_dbscan(skills)
                
                if not clusters:
                    print_current("â„¹ï¸  æœªæ‰¾åˆ°skillèšç±»ï¼Œå°è¯•è®©LLMè¯„ä¼°æ‰€æœ‰skill...")
                    self.logger.info("No skill clusters found by DBSCAN, trying LLM-based integration for all skills")
                    
                    # å¤‡é€‰æ–¹æ¡ˆï¼šè®©LLMåˆ¤æ–­æ‰€æœ‰skillæ˜¯å¦å¯ä»¥æ•´åˆ
                    # ä½†åªå°è¯•è¾ƒå°çš„skillç»„ï¼ˆ2-4ä¸ªï¼‰ï¼Œé¿å…tokenè¿‡å¤š
                    if len(skills) >= 2 and len(skills) <= 6:
                        # å°è¯•å°†æ‰€æœ‰skillä½œä¸ºä¸€ä¸ªç»„è®©LLMåˆ¤æ–­
                        cluster_skills = []
                        skill_titles = []
                        for idx, (file_path, skill_data) in enumerate(skills):
                            front_matter = skill_data['front_matter']
                            skill_titles.append(front_matter.get('title', 'Unknown'))
                            cluster_skills.append({
                                'file_path': file_path,
                                'front_matter': front_matter,
                                'content': skill_data['content']
                            })
                        
                        print_current(f"ğŸ¤” è¯·æ±‚LLMè¯„ä¼° {len(cluster_skills)} ä¸ªskillæ˜¯å¦åº”è¯¥æ•´åˆ...")
                        
                        decision = self._call_llm_for_merge_decision(cluster_skills)
                        
                        print_current(f"ğŸ¤– LLMå†³ç­–: {'âœ… åˆå¹¶' if decision.get('should_merge') else 'âŒ ä¸åˆå¹¶'}")
                        if decision.get('reason'):
                            print_current(f"   ç†ç”±: {decision.get('reason')[:200]}")
                        
                        if decision.get('should_merge'):
                            # åˆ›å»ºæ•´åˆåçš„skill
                            skill_id = str(int(time.time()))
                            title = decision.get('title', f"Integrated Skill from {len(cluster_skills)} tasks")
                            usage_conditions = decision.get('usage_conditions', '')
                            content = decision.get('content', '')
                            
                            # åˆå¹¶task_directorieså’Œfetch_count
                            merged_task_dirs = []
                            merged_fetch_count = 0
                            qualities = []
                            
                            for skill in cluster_skills:
                                front_matter = skill['front_matter']
                                merged_task_dirs.extend(front_matter.get('task_directories', []))
                                merged_fetch_count += front_matter.get('fetch_count', 0)
                                qualities.append(front_matter.get('quality_index', 0.5))
                            
                            # åˆ›å»ºæ–°skill
                            front_matter = {
                                'skill_id': skill_id,
                                'title': title,
                                'usage_conditions': usage_conditions,
                                'quality_index': round(sum(qualities) / len(qualities), 3),
                                'fetch_count': merged_fetch_count,
                                'related_code': '',
                                'task_directories': list(set(merged_task_dirs)),
                                'created_at': datetime.now().isoformat(),
                                'updated_at': datetime.now().isoformat(),
                                'last_used_at': None,
                                'user_preferences': ''
                            }
                            
                            safe_title = self.skill_tools._sanitize_filename(title)
                            # ä½¿ç”¨skill_adv_å‰ç¼€æ ‡è®°ä¸ºé«˜çº§æ•´åˆskill
                            skill_filename = f"skill_adv_{safe_title}.md"
                            skill_file_path = os.path.join(self.skill_tools.experience_dir, skill_filename)
                            
                            if os.path.exists(skill_file_path):
                                name, ext = os.path.splitext(skill_filename)
                                skill_filename = f"{name}_{skill_id}{ext}"
                                skill_file_path = os.path.join(self.skill_tools.experience_dir, skill_filename)
                            
                            # è®°å½•æ¥æºskillçš„ID
                            source_skill_ids = [str(skill['front_matter'].get('skill_id', '')) for skill in cluster_skills]
                            front_matter['source_skill_ids'] = source_skill_ids
                            
                            self.skill_tools._save_skill_file(skill_file_path, front_matter, content)
                            print_current(f"âœ… å·²åˆ›å»ºæ•´åˆskill: {skill_filename}")
                            print_current(f"   æ ‡é¢˜: {title}")
                            print_current(f"   æ¥æº: {len(cluster_skills)} ä¸ªåŸå§‹skill")
                            
                            integrated_count = 1
                        else:
                            reason = decision.get('reason', 'æœªæä¾›ç†ç”±')
                            print_current(f"â­ï¸  LLMå†³å®šä¸åˆå¹¶: {reason[:150]}")
                            integrated_count = 0
                    else:
                        integrated_count = 0
                
                # å¤„ç†æ‰¾åˆ°çš„èšç±»
                if clusters:
                    for cluster_id, indices in clusters.items():
                        if len(indices) < 2:
                            continue
                        
                        # è·å–èšç±»ä¸­çš„skill
                        cluster_skills = []
                        skill_titles = []
                        for idx in indices:
                            file_path, skill_data = skills[idx]
                            front_matter = skill_data['front_matter']
                            skill_titles.append(front_matter.get('title', 'Unknown'))
                            cluster_skills.append({
                                'file_path': file_path,
                                'front_matter': front_matter,
                                'content': skill_data['content']
                            })
                        
                        print_current(f"ğŸ“¦ å¤„ç†èšç±» {cluster_id} ({len(cluster_skills)} ä¸ªskill): {', '.join([t[:30] + '...' if len(t) > 30 else t for t in skill_titles])}")
                        
                        # LLMå†³ç­–
                        decision = self._call_llm_for_merge_decision(cluster_skills)
                        
                        print_current(f"ğŸ¤– LLMå†³ç­–: {'âœ… åˆå¹¶' if decision.get('should_merge') else 'âŒ ä¸åˆå¹¶'}")
                        if decision.get('reason'):
                            print_current(f"   ç†ç”±: {decision.get('reason')[:200]}")
                        
                        if decision.get('should_merge'):
                            # åˆ›å»ºæ–°çš„ç»¼åˆskill
                            skill_id = str(int(time.time()))
                            title = decision.get('title', f"Integrated Skill {cluster_id}")
                            usage_conditions = decision.get('usage_conditions', '')
                            content = decision.get('content', '')
                            
                            # åˆå¹¶task_directorieså’Œfetch_count
                            merged_task_dirs = []
                            merged_fetch_count = 0
                            qualities = []
                            
                            for skill in cluster_skills:
                                front_matter = skill['front_matter']
                                merged_task_dirs.extend(front_matter.get('task_directories', []))
                                merged_fetch_count += front_matter.get('fetch_count', 0)
                                qualities.append(front_matter.get('quality_index', 0.5))
                            
                            # è®°å½•æ¥æºskillçš„ID
                            source_skill_ids = [str(skill['front_matter'].get('skill_id', '')) for skill in cluster_skills]
                            
                            # åˆ›å»ºæ–°skill
                            front_matter = {
                                'skill_id': skill_id,
                                'title': title,
                                'usage_conditions': usage_conditions,
                                'quality_index': round(sum(qualities) / len(qualities), 3),
                                'fetch_count': merged_fetch_count,
                                'related_code': '',
                                'task_directories': list(set(merged_task_dirs)),
                                'source_skill_ids': source_skill_ids,  # è®°å½•æ¥æºskill
                                'created_at': datetime.now().isoformat(),
                                'updated_at': datetime.now().isoformat(),
                                'last_used_at': None,
                                'user_preferences': ''
                            }
                            
                            safe_title = self.skill_tools._sanitize_filename(title)
                            # ä½¿ç”¨skill_adv_å‰ç¼€æ ‡è®°ä¸ºé«˜çº§æ•´åˆskill
                            skill_filename = f"skill_adv_{safe_title}.md"
                            skill_file_path = os.path.join(self.skill_tools.experience_dir, skill_filename)
                            
                            if os.path.exists(skill_file_path):
                                name, ext = os.path.splitext(skill_filename)
                                skill_filename = f"{name}_{skill_id}{ext}"
                                skill_file_path = os.path.join(self.skill_tools.experience_dir, skill_filename)
                            
                            self.skill_tools._save_skill_file(skill_file_path, front_matter, content)
                            print_current(f"âœ… å·²åˆ›å»ºæ•´åˆskill: {skill_filename}")
                            print_current(f"   æ ‡é¢˜: {title}")
                            print_current(f"   æ¥æº: {len(cluster_skills)} ä¸ªåŸå§‹skill")
                            
                            integrated_count += 1
                        else:
                            reason = decision.get('reason', 'æœªæä¾›ç†ç”±')
                            print_current(f"â­ï¸  èšç±» {cluster_id} æœªåˆå¹¶: {reason[:150]}")
            
            self.logger.info(f"Integrated {integrated_count} skill clusters")
            print_current(f"âœ… Integrated {integrated_count} skill clusters")
        
        # 3. æ¸…ç†é•¿æœŸä¸ä½¿ç”¨çš„skill
        print_current("Step 3: Cleaning unused skills...")
        skills = self._load_all_skills()
        cleaned_count = self._clean_unused_skills(skills)
        self.logger.info(f"Cleaned {cleaned_count} unused skills")
        print_current(f"âœ… Cleaned {cleaned_count} unused skills")
        
        self.logger.info("Skill management process completed")
        print_system("âœ… Skill management process completed")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Skill management script')
    parser.add_argument('--root-dir', type=str, help='Root directory for data (overrides config)')
    parser.add_argument('--config', type=str, default='config/config.txt', help='Config file path')
    
    args = parser.parse_args()
    
    manager = SkillManager(root_dir=args.root_dir, config_file=args.config)
    manager.run()


if __name__ == '__main__':
    main()

