#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Copyright (c) 2025 AGI Agent Research Group.

LLM Summary Compressor - Use LLM to summarize old conversation history:
1. When history exceeds trigger_length, use LLM to summarize old records
2. Keep the most recent N rounds uncompressed
3. Replace old records with a concise summary
"""

from typing import Dict, Any, List, Tuple, Optional
from .print_system import print_current, print_debug, streaming_context


class LLMSummaryCompressor:
    """
    LLM Summary Compressor
    
    Uses LLM to generate summaries of old conversation history instead of deleting them.
    This preserves important context while reducing token consumption.
    """
    
    def __init__(
        self,
        trigger_length: Optional[int] = None,
        target_length: Optional[int] = None,
        keep_recent_rounds: int = 2,
        api_client=None,
        model: str = None,
        api_key: str = None,
        api_base: str = None,
    ):
        """
        Initialize the LLM summary compressor
        
        Args:
            trigger_length: Length threshold for triggering compression (default: from config or 100000)
            target_length: Target length after compression (default: from config or 50000)
            keep_recent_rounds: Number of most recent rounds to keep uncompressed (default: 2)
            api_client: Optional pre-configured API client
            model: Model name for LLM calls
            api_key: API key for LLM calls
            api_base: API base URL for LLM calls
        """
        # Load trigger_length from config
        if trigger_length is None:
            try:
                from config_loader import get_summary_trigger_length
                trigger_length = get_summary_trigger_length()
            except (ImportError, Exception) as e:
                print_debug(f"âš ï¸ Failed to load summary_trigger_length from config: {e}, using default 100000")
                trigger_length = 100000
        
        # Load target_length from config
        if target_length is None:
            try:
                from config_loader import get_compression_target_length
                target_length = get_compression_target_length()
            except (ImportError, Exception) as e:
                target_length = int(trigger_length * 0.5)
                print_debug(f"âš ï¸ Failed to load compression_target_length from config: {e}, using default {target_length}")
        
        self.trigger_length = trigger_length
        self.target_length = target_length
        self.keep_recent_rounds = keep_recent_rounds
        self.api_client = api_client
        self.model = model
        self.api_key = api_key
        self.api_base = api_base
        
        # Load max_tokens from config
        try:
            from config_loader import get_max_tokens
            self.max_tokens = get_max_tokens() or 16384
        except (ImportError, Exception):
            self.max_tokens = 16384
        
        # Load summary_streaming from config
        try:
            from config_loader import get_summary_streaming
            self.streaming = get_summary_streaming()
        except (ImportError, Exception):
            self.streaming = True  # Default to streaming enabled
    
    def compress_history(
        self, task_history: List[Dict[str, Any]]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        Perform compression using LLM summarization
        
        Args:
            task_history: Original history records
            
        Returns:
            (compressed_history, stats): The compressed history and statistics
        """
        if not task_history:
            return task_history, {
                "compression_method": "llm_summary",
                "compressed": False,
                "original_records": 0,
                "final_records": 0,
            }
        
        # Calculate total length
        total_length = self._calculate_total_length(task_history)
        
        # Check if compression is needed
        if total_length <= self.trigger_length:
            print_debug(f"ğŸ—œï¸ [LLM Summary] History length {total_length:,} <= trigger_length {self.trigger_length:,}, skipping compression")
            return task_history, {
                "compression_method": "llm_summary",
                "compressed": False,
                "original_records": len(task_history),
                "final_records": len(task_history),
                "original_length": total_length,
                "final_length": total_length,
            }
        
        # Filter records with results (LLM records)
        history_for_llm = [r for r in task_history if "result" in r or "error" in r]
        non_llm_records = [r for r in task_history if not ("result" in r or "error" in r)]
        
        # Not enough records to compress (need at least 1 record when keep_recent_rounds=0)
        min_records = max(1, self.keep_recent_rounds + 1)
        if len(history_for_llm) < min_records:
            return task_history, {
                "compression_method": "llm_summary",
                "compressed": False,
                "original_records": len(task_history),
                "final_records": len(task_history),
                "reason": "not_enough_records"
            }
        
        # Split into old records (to summarize) and recent records (to keep)
        # Handle keep_recent_rounds=0 specially (Python slice [:-0] returns empty list)
        if self.keep_recent_rounds == 0:
            records_to_summarize = history_for_llm  # Compress all records
            recent_records = []  # Keep nothing
        else:
            records_to_summarize = history_for_llm[:-self.keep_recent_rounds]
            recent_records = history_for_llm[-self.keep_recent_rounds:]
        
        if not records_to_summarize:
            return task_history, {
                "compression_method": "llm_summary",
                "compressed": False,
                "original_records": len(task_history),
                "final_records": len(task_history),
                "reason": "no_records_to_summarize"
            }
        
        # Check for existing summary in records to summarize (for incremental update)
        existing_summary = None
        new_records = []
        for record in records_to_summarize:
            if record.get("type") == "llm_summary":
                # Extract existing summary content (remove the reminder header)
                result = record.get("result", "")
                # Find the actual summary content after the reminder box
                if "â•š" in result:
                    existing_summary = result.split("â•š")[1].split("â•")[-1].strip()
                elif "å†å²å¯¹è¯æ‘˜è¦" in result:
                    existing_summary = result.split("\n\n", 2)[-1] if "\n\n" in result else result
                else:
                    existing_summary = result
                print_current(f"ğŸ—œï¸ [LLM Summary] Found existing summary ({len(existing_summary):,} chars), will perform incremental update")
            else:
                new_records.append(record)
        
        # Use new_records (without old summary) for summarization
        records_for_summary = new_records if new_records else records_to_summarize
        
        # Calculate original length of records to summarize
        original_summary_length = self._calculate_total_length(records_for_summary)
        recent_length = self._calculate_total_length(recent_records)
        
        # Calculate target summary length
        # target_length is the overall target, minus the length of recent records we're keeping
        target_summary_length = max(self.target_length - recent_length, int(original_summary_length * 0.2))
        
        print_current(f"ğŸ—œï¸ [LLM Summary] Compressing {len(records_for_summary)} records ({original_summary_length:,} chars) using LLM summarization...")
        print_current(f"ğŸ—œï¸ [LLM Summary] Target summary length: ~{target_summary_length:,} chars")
        
        try:
            # Generate LLM summary with target length (pass existing summary for incremental update)
            summary = self._generate_summary(records_for_summary, target_summary_length, existing_summary)
            
            if not summary:
                print_debug("âš ï¸ [LLM Summary] Failed to generate summary, keeping original history")
                return task_history, {
                    "compression_method": "llm_summary",
                    "compressed": False,
                    "error": "summary_generation_failed"
                }
            
            # Create summary record with file reading reminder - VERY PROMINENT
            file_reading_reminder = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âš ï¸âš ï¸âš ï¸ ã€å¼ºåˆ¶è¦æ±‚ - å¿…é¡»å…ˆè¯»å–æ–‡ä»¶å†ç»§ç»­ç¼–ç ã€‘ âš ï¸âš ï¸âš ï¸           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  æ­¤æ‘˜è¦æ˜¯å‹ç¼©ç‰ˆæœ¬ï¼Œä»£ç ç»†èŠ‚ä¸å®Œæ•´ï¼                                â•‘
â•‘                                                                    â•‘
â•‘  åœ¨æ‰§è¡Œä»»ä½•ç¼–ç æ“ä½œä¹‹å‰ï¼Œä½ å¿…é¡»ï¼š                                  â•‘
â•‘  1. ç«‹å³ä½¿ç”¨ read_file å·¥å…·è¯»å–ä¸‹æ–¹ FILES TO READ ä¸­çš„æ–‡ä»¶        â•‘
â•‘  2. ç¡®è®¤ç°æœ‰ä»£ç çš„å˜é‡åã€å‡½æ•°ç­¾åã€ç±»ç»“æ„                        â•‘
â•‘  3. ç¡®ä¿æ–°ä»£ç ä¸ç°æœ‰ä»£ç å…¼å®¹                                      â•‘
â•‘                                                                    â•‘
â•‘  âŒ ä¸è¯»å–æ–‡ä»¶ç›´æ¥ç¼–ç  = ä»£ç ä¸å…¼å®¹ã€å˜é‡åé”™è¯¯ã€å¯¼å…¥ç¼ºå¤±         â•‘
â•‘  âœ… å…ˆè¯»å–å…³é”®æ–‡ä»¶ = ä»£ç æ­£ç¡®ã€æ— å†²çª                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"""
            summary_record = {
                "result": f"[ğŸ“‹ å†å²å¯¹è¯æ‘˜è¦ - å·²å‹ç¼© {len(records_to_summarize)} æ¡è®°å½•]\n\n{file_reading_reminder}{summary}",
                "type": "llm_summary",
                "compressed_records_count": len(records_to_summarize),
                "original_length": original_summary_length,
            }
            
            # Build final history: non-LLM records + summary + recent records
            final_history = non_llm_records + [summary_record] + recent_records
            
            # Calculate final length
            final_length = self._calculate_total_length(final_history)
            summary_length = len(summary)
            
            print_current(f"ğŸ—œï¸ [LLM Summary] Compression complete: {original_summary_length:,} chars â†’ {summary_length:,} chars (summary), kept {len(recent_records)} recent rounds ({recent_length:,} chars)")
            
            # Print summary content to log for debugging
            print_current(f"ğŸ—œï¸ [LLM Summary] ========== æ‘˜è¦å†…å®¹å¼€å§‹ ==========")
            print_current(summary)
            print_current(f"ğŸ—œï¸ [LLM Summary] ========== æ‘˜è¦å†…å®¹ç»“æŸ (å…± {summary_length:,} å­—ç¬¦) ==========")
            
            stats = {
                "compression_method": "llm_summary",
                "compressed": True,
                "original_records": len(task_history),
                "final_records": len(final_history),
                "records_summarized": len(records_to_summarize),
                "recent_rounds_kept": len(recent_records),
                "original_length": total_length,
                "final_length": final_length,
                "summary_length": summary_length,
                "compression_ratio": f"{(1 - final_length/total_length)*100:.1f}%" if total_length > 0 else "0%",
            }
            
            return final_history, stats
            
        except Exception as e:
            print_debug(f"âš ï¸ [LLM Summary] Compression failed: {e}")
            import traceback
            traceback.print_exc()
            return task_history, {
                "compression_method": "llm_summary",
                "compressed": False,
                "error": str(e)
            }
    
    def _generate_summary(self, records: List[Dict[str, Any]], target_length: int = None, existing_summary: str = None) -> str:
        """
        Generate summary using LLM (supports incremental update)
        
        Args:
            records: Records to summarize
            target_length: Target length of the summary in characters
            existing_summary: Previous summary to update (for incremental compression)
            
        Returns:
            Summary text
        """
        # Prepare content to summarize - no truncation, keep all content
        content_parts = []
        original_length = 0
        for i, record in enumerate(records, 1):
            result = record.get("result", "")
            if result:
                result_str = str(result)
                original_length += len(result_str)
                content_parts.append(f"[è®°å½• {i}]\n{result_str}")
        
        content_to_summarize = "\n\n---\n\n".join(content_parts)
        
        # Calculate target length if not provided
        if target_length is None:
            target_length = int(original_length * 0.2)  # Default to 20% of original
        
        # Build prompts with structured format
        min_length = int(target_length * 0.8)
        
        # Determine if this is an incremental update
        is_incremental = existing_summary is not None and len(existing_summary) > 100
        
        if is_incremental:
            # Calculate existing summary length for compression guidance
            existing_len = len(existing_summary) if existing_summary else 0
            max_length = int(target_length * 1.2)
            
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè½¯ä»¶å·¥ç¨‹å¯¹è¯æ‘˜è¦å™¨ï¼Œç”¨äº IDE åœºæ™¯ã€‚ä½ éœ€è¦å¯¹å·²æœ‰çš„"é¡¹ç›®çŠ¶æ€æ‘˜è¦"è¿›è¡Œ**å¢é‡æ›´æ–°ä¸å‹ç¼©**ã€‚

âš ï¸ **ã€å…³é”®ï¼šæ§åˆ¶é•¿åº¦ï¼Œé˜²æ­¢è†¨èƒ€ã€‘** âš ï¸
- å½“å‰å·²æœ‰æ‘˜è¦ï¼š{existing_len:,} å­—ç¬¦
- **ç›®æ ‡é•¿åº¦ï¼š{target_length:,} å­—ç¬¦**
- **æœ€å¤§é•¿åº¦ï¼š{max_length:,} å­—ç¬¦ï¼ˆä¸¥ç¦è¶…è¿‡ï¼‰**

å¦‚æœåˆå¹¶åè¶…è¿‡ç›®æ ‡é•¿åº¦ï¼Œä½ å¿…é¡»**å‹ç¼©æ—§å†…å®¹**ï¼š

========================
å‹ç¼©ç­–ç•¥ï¼ˆé‡è¦ï¼‰
========================

1. **åˆå¹¶ç›¸ä¼¼ TASK**ï¼š
   - åŒä¸€æ¨¡å—/åŠŸèƒ½çš„å¤šä¸ª TASK å¯ä»¥åˆå¹¶ä¸ºä¸€ä¸ª
   - ä¾‹å¦‚ï¼šTASK 3ï¼ˆåˆ›å»ºæ¨¡å‹ï¼‰+ TASK 5ï¼ˆæ›´æ–°æ¨¡å‹ï¼‰â†’ åˆå¹¶ä¸ºä¸€ä¸ª"æ¨¡å‹å¼€å‘"TASK

2. **ç²¾ç®€ DETAILS**ï¼š
   - åªä¿ç•™å…³é”®å†³ç­–å’Œæœ€ç»ˆç»“æœ
   - åˆ é™¤ä¸­é—´è¿‡ç¨‹å’Œé‡å¤æè¿°
   - ä¾‹å¦‚ï¼š"åˆ›å»ºäº†æ–‡ä»¶å¹¶æ·»åŠ äº†ä»£ç " â†’ "åˆ›å»º xxx.py"

3. **ç²¾ç®€ CODE SNIPPETS**ï¼š
   - åªä¿ç•™**å‡½æ•°ç­¾å**å’Œ**å…³é”®é€»è¾‘**
   - åˆ é™¤å®Œæ•´ä»£ç å®ç°ï¼Œä¿ç•™ç»“æ„

4. **å¿…é¡»ä¿ç•™**ï¼ˆä¸å¯åˆ é™¤ï¼‰ï¼š
   - æ‰€æœ‰ FILEPATHSï¼ˆæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼‰
   - ERRORS AND SOLUTIONS
   - CONFIGURATION
   - FILES TO READ

========================
å¢é‡æ›´æ–°è§„åˆ™
========================

A. ä»»åŠ¡åˆå¹¶
- æ–°å¯¹è¯å±äºå·²æœ‰ TASK â†’ æ›´æ–°è¯¥ TASK
- æ–°å¯¹è¯æ˜¯æ–°ä»»åŠ¡ â†’ åˆ›å»ºæ–° TASKï¼ˆè€ƒè™‘æ˜¯å¦å¯ä¸å·²æœ‰ TASK åˆå¹¶ï¼‰

B. çŠ¶æ€æ›´æ–°
- doneï¼šå·²å®Œæˆ
- in-progressï¼šè¿›è¡Œä¸­

C. é•¿åº¦æ§åˆ¶
- è¾“å‡ºé•¿åº¦å¿…é¡»åœ¨ **{min_length:,} ~ {max_length:,} å­—ç¬¦ä¹‹é—´**
- å¦‚æœè¶…è¿‡ {max_length:,}ï¼Œå¿…é¡»è¿›ä¸€æ­¥å‹ç¼©æ—§ TASK çš„ DETAILS"""
        else:
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè½¯ä»¶å·¥ç¨‹å¯¹è¯æ‘˜è¦å™¨ï¼Œç”¨äº IDE åœºæ™¯ã€‚ä½ éœ€è¦å°†å¤šè½®å¯¹è¯å†å²æ•´ç†æˆç»“æ„åŒ–çš„"é¡¹ç›®çŠ¶æ€æ‘˜è¦ï¼ˆProject Summaryï¼‰"ã€‚

========================
è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
========================

TASK <ç¼–å·>: <ä»»åŠ¡å>
STATUS: done | in-progress | abandoned
USER QUERIES: <è½®æ¬¡èŒƒå›´ï¼Œå¦‚ 1-5>
DETAILS:
- <è¦ç‚¹1>
- <è¦ç‚¹2>
- <å…³é”®å†³ç­–ã€å˜æ›´ã€ç»“æœ>
FILEPATHS:
- <åˆ›å»º/ä¿®æ”¹çš„æ–‡ä»¶è·¯å¾„1>
- <åˆ›å»º/ä¿®æ”¹çš„æ–‡ä»¶è·¯å¾„2>
CODE SNIPPETS:
```<è¯­è¨€>
<å…³é”®ä»£ç ç‰‡æ®µ>
```

ï¼ˆå¯ä»¥æœ‰å¤šä¸ª TASKï¼ŒæŒ‰æ—¶é—´é¡ºåºç¼–å·ï¼‰

ERRORS AND SOLUTIONS:
- <é”™è¯¯1>: <è§£å†³æ–¹æ¡ˆ1>
- <é”™è¯¯2>: <è§£å†³æ–¹æ¡ˆ2>

USER CORRECTIONS AND INSTRUCTIONS:
- <ç”¨æˆ·çš„ç¡¬æ€§è¦æ±‚æ¸…å•>

CONFIGURATION:
- <é…ç½®é¡¹1>: <å€¼1>
- <é…ç½®é¡¹2>: <å€¼2>

NEXT STEPS:
- <ä¸‹ä¸€æ­¥åŠ¨ä½œ>

âš ï¸ FILES TO READ (é‡è¦):
- <åç»­ç¼–ç å‰å¿…é¡»è¯»å–çš„æ–‡ä»¶1>
- <åç»­ç¼–ç å‰å¿…é¡»è¯»å–çš„æ–‡ä»¶2>
ï¼ˆåˆ—å‡ºæ‰€æœ‰åŒ…å«å…³é”®ä»£ç ç»“æ„çš„æ–‡ä»¶ï¼Œåç»­æ¨¡å‹éœ€è¦è¯»å–è¿™äº›æ–‡ä»¶æ¥äº†è§£ä»£ç ä¸Šä¸‹æ–‡ï¼‰

========================
æ‘˜è¦è§„åˆ™ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
========================

A. ä»»åŠ¡è¯†åˆ«ä¸ç»„ç»‡
1) è¯†åˆ«å¯¹è¯ä¸­çš„ä¸åŒä»»åŠ¡/ç›®æ ‡ï¼Œæ¯ä¸ªç‹¬ç«‹ç›®æ ‡åˆ›å»ºä¸€ä¸ª TASK
2) åŒä¸€ç›®æ ‡/åŒä¸€æ¨¡å—/åŒä¸€æ–‡ä»¶çš„æ“ä½œå½’å…¥åŒä¸€ä¸ª TASK
3) æŒ‰æ—¶é—´é¡ºåºç¼–å·ï¼šTASK 1, TASK 2, ...

B. çŠ¶æ€åˆ¤å®š
- doneï¼šç›®æ ‡å·²å®ç°æˆ–ç”¨æˆ·ç¡®è®¤å®Œæˆ
- in-progressï¼šå·²å¼€å§‹ä½†ä»éœ€åç»­å·¥ä½œ

C. å†…å®¹è¯¦ç»†åº¦è¦æ±‚
- DETAILS å¿…é¡»åŒ…å«å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
- FILEPATHS å¿…é¡»åˆ—å‡ºæ‰€æœ‰æ¶‰åŠçš„æ–‡ä»¶å®Œæ•´è·¯å¾„
- CODE SNIPPETS å¿…é¡»ä¿ç•™å…³é”®ä»£ç ï¼ˆå‡½æ•°ç­¾åã€ç±»å®šä¹‰ã€é…ç½®å†…å®¹ï¼‰
- æ¯ä¸ª TASK çš„å†…å®¹è¦å……åˆ†å±•å¼€

D. é”™è¯¯ä¸é—®é¢˜è®°å½•
- æ‰€æœ‰å¼‚å¸¸ã€æŠ¥é”™ã€å¤±è´¥å¿…é¡»è®°å½•åˆ° ERRORS AND SOLUTIONS
- å¿…é¡»åŒæ—¶è®°å½•è§£å†³æ–¹æ¡ˆ

E. ä¸è¦ç¼–é€ 
- æ²¡æœ‰åœ¨å¯¹è¯ä¸­å‡ºç°çš„ä¿¡æ¯ä¸è¦å†™
- ä¸ç¡®å®šçš„å†…å®¹ä¸è¦ä¸‹ç»“è®º

F. FILES TO READï¼ˆå¿…é¡»å¡«å†™ï¼‰
- åˆ—å‡ºæ‰€æœ‰åŒ…å«å…³é”®ä»£ç ç»“æ„çš„æ–‡ä»¶ï¼ˆå¦‚å®šä¹‰äº†ç±»ã€å‡½æ•°ã€é…ç½®çš„æ–‡ä»¶ï¼‰
- è¿™äº›æ–‡ä»¶æ˜¯åç»­ç¼–ç æ—¶å¿…é¡»å…ˆè¯»å–çš„ï¼Œä»¥ç¡®ä¿ä»£ç å…¼å®¹æ€§
- ä¼˜å…ˆåˆ—å‡ºï¼šä¸»ç¨‹åºæ–‡ä»¶ã€æ¨¡å‹å®šä¹‰ã€é…ç½®æ–‡ä»¶ã€API è·¯ç”±æ–‡ä»¶

G. é•¿åº¦è¦æ±‚
- è¾“å‡ºå¿…é¡»è¾¾åˆ° **{min_length:,} å­—ç¬¦ä»¥ä¸Š**
- ç›®æ ‡é•¿åº¦ï¼š**{target_length:,} å­—ç¬¦**
- é€šè¿‡è¯¦ç»†å±•å¼€æ¯ä¸ª TASK çš„ DETAILS å’Œ CODE SNIPPETS æ¥è¾¾åˆ°é•¿åº¦è¦æ±‚"""

        if is_incremental:
            user_prompt = f"""è¯·å¯¹ä»¥ä¸‹é¡¹ç›®çŠ¶æ€æ‘˜è¦è¿›è¡Œ**å¢é‡æ›´æ–°ä¸å‹ç¼©**ã€‚

âš ï¸ **ã€é•¿åº¦æ§åˆ¶ - ä¸¥æ ¼æ‰§è¡Œã€‘** âš ï¸
- å·²æœ‰æ‘˜è¦ï¼š{existing_len:,} å­—ç¬¦
- ç›®æ ‡é•¿åº¦ï¼š**{target_length:,} å­—ç¬¦**
- æœ€å¤§é•¿åº¦ï¼š**{max_length:,} å­—ç¬¦ï¼ˆä¸¥ç¦è¶…è¿‡ï¼‰**

å¦‚æœåˆå¹¶åè¶…é•¿ï¼Œè¯·å‹ç¼©æ—§ TASK çš„ DETAILSï¼ˆç²¾ç®€æè¿°ï¼‰ï¼Œä½†ä¿ç•™æ‰€æœ‰ FILEPATHSã€‚

========================
å·²æœ‰æ‘˜è¦ï¼ˆå¯ä»¥å‹ç¼© DETAILSï¼Œä½†ä¿ç•™ FILEPATHSï¼‰
========================

{existing_summary}

========================
æ–°å¢å¯¹è¯ç‰‡æ®µ
========================

{content_to_summarize}

========================

è¯·è¾“å‡º**æ›´æ–°åçš„æ‘˜è¦**ï¼ˆåˆå¹¶æ–°æ—§å†…å®¹ï¼Œæ§åˆ¶åœ¨ {target_length:,} å­—ç¬¦å·¦å³ï¼Œæœ€å¤š {max_length:,} å­—ç¬¦ï¼‰ï¼š"""
        else:
            user_prompt = f"""è¯·å°†ä»¥ä¸‹ {len(records)} æ¡å¯¹è¯å†å²æ•´ç†æˆç»“æ„åŒ–çš„é¡¹ç›®çŠ¶æ€æ‘˜è¦ã€‚

ã€é•¿åº¦è¦æ±‚ã€‘è¾“å‡ºå¿…é¡»è¾¾åˆ° {min_length:,} å­—ç¬¦ä»¥ä¸Šï¼Œç›®æ ‡ {target_length:,} å­—ç¬¦

ä»¥ä¸‹æ˜¯éœ€è¦æ•´ç†çš„ {len(records)} æ¡è®°å½•ï¼š

{content_to_summarize}

è¯·æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºé¡¹ç›®çŠ¶æ€æ‘˜è¦ï¼ˆåŒ…å« TASKã€STATUSã€DETAILSã€FILEPATHSã€CODE SNIPPETS ç­‰ï¼‰ï¼š"""

        # Call LLM
        return self._call_llm(system_prompt, user_prompt)
    
    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """
        Call LLM to generate response (supports streaming output)
        
        Args:
            system_prompt: System prompt
            user_prompt: User prompt
            
        Returns:
            LLM response text
        """
        try:
            # Method 1: Use provided API client (non-streaming for backward compatibility)
            if self.api_client:
                # Check if it's Anthropic client
                if hasattr(self.api_client, 'messages'):
                    response = self.api_client.messages.create(
                        model=self.model,
                        max_tokens=self.max_tokens,
                        system=system_prompt,
                        messages=[{"role": "user", "content": user_prompt}]
                    )
                    if hasattr(response, 'content') and response.content:
                        if isinstance(response.content, list):
                            return response.content[0].text
                        return str(response.content)
                # Check if it's OpenAI-compatible client
                elif hasattr(self.api_client, 'chat'):
                    response = self.api_client.chat.completions.create(
                        model=self.model,
                        max_tokens=self.max_tokens,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ]
                    )
                    return response.choices[0].message.content
            
            # Method 2: Create new client using config
            from config_loader import get_api_key, get_api_base, get_model
            
            api_key = self.api_key or get_api_key()
            api_base = self.api_base or get_api_base()
            model = self.model or get_model()
            
            if not api_key or not api_base or not model:
                raise ValueError("Missing API configuration for LLM summary compression")
            
            # Determine API type based on api_base
            is_anthropic = api_base.lower().endswith('/anthropic') if api_base else False
            
            print_current(f"ğŸ—œï¸ [LLM Summary] Calling LLM with max_tokens={self.max_tokens}, model={model}, streaming={self.streaming}")
            
            if is_anthropic:
                return self._call_anthropic(api_key, api_base, model, system_prompt, user_prompt)
            else:
                return self._call_openai(api_key, api_base, model, system_prompt, user_prompt)
            
        except Exception as e:
            print_debug(f"âš ï¸ [LLM Summary] LLM call failed: {e}")
            raise e
    
    def _call_anthropic(self, api_key: str, api_base: str, model: str, 
                        system_prompt: str, user_prompt: str) -> str:
        """
        Call Anthropic API (supports streaming)
        """
        from anthropic import Anthropic
        
        client = Anthropic(api_key=api_key, base_url=api_base)
        
        if self.streaming:
            # Streaming mode
            content = ""
            with streaming_context(show_start_message=False) as printer:
                printer.write("\nğŸ—œï¸ [æ‘˜è¦ç”Ÿæˆä¸­] ")
                
                with client.messages.stream(
                    model=model,
                    max_tokens=self.max_tokens,
                    system=system_prompt,
                    messages=[{"role": "user", "content": user_prompt}]
                ) as stream:
                    stop_reason = None
                    input_tokens = 0
                    output_tokens = 0
                    
                    for event in stream:
                        event_type = getattr(event, 'type', None)
                        
                        # Handle content_block_delta event (text content)
                        if event_type == "content_block_delta":
                            delta = getattr(event, 'delta', None)
                            if delta:
                                delta_type = getattr(delta, 'type', None)
                                if delta_type == "text_delta":
                                    text = getattr(delta, 'text', '')
                                    content += text
                                    printer.write(text)
                        
                        # Handle message_delta event (usage stats)
                        elif event_type == "message_delta":
                            delta = getattr(event, 'delta', None)
                            if delta:
                                stop_reason = getattr(delta, 'stop_reason', stop_reason)
                            usage = getattr(event, 'usage', None)
                            if usage:
                                output_tokens = getattr(usage, 'output_tokens', output_tokens)
                        
                        # Handle message_start event (input tokens)
                        elif event_type == "message_start":
                            message = getattr(event, 'message', None)
                            if message:
                                usage = getattr(message, 'usage', None)
                                if usage:
                                    input_tokens = getattr(usage, 'input_tokens', input_tokens)
                
                printer.write("\n")
            
            # Print API response metadata
            print_current(f"ğŸ—œï¸ [LLM Summary] API Response - stop_reason: {stop_reason}, input_tokens: {input_tokens}, output_tokens: {output_tokens}")
            
            if not content:
                raise ValueError("Empty response from Anthropic API (streaming)")
            return content
        else:
            # Non-streaming mode
            response = client.messages.create(
                model=model,
                max_tokens=self.max_tokens,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            stop_reason = getattr(response, 'stop_reason', 'unknown')
            usage = getattr(response, 'usage', None)
            if usage:
                print_current(f"ğŸ—œï¸ [LLM Summary] API Response - stop_reason: {stop_reason}, input_tokens: {getattr(usage, 'input_tokens', 'N/A')}, output_tokens: {getattr(usage, 'output_tokens', 'N/A')}")
            else:
                print_current(f"ğŸ—œï¸ [LLM Summary] API Response - stop_reason: {stop_reason}")
            
            if hasattr(response, 'content') and response.content:
                if isinstance(response.content, list):
                    return response.content[0].text
                return str(response.content)
            raise ValueError("Empty response from Anthropic API")
    
    def _call_openai(self, api_key: str, api_base: str, model: str,
                     system_prompt: str, user_prompt: str) -> str:
        """
        Call OpenAI-compatible API (supports streaming)
        """
        from openai import OpenAI
        
        client = OpenAI(api_key=api_key, base_url=api_base)
        
        if self.streaming:
            # Streaming mode
            content = ""
            with streaming_context(show_start_message=False) as printer:
                printer.write("\nğŸ—œï¸ [æ‘˜è¦ç”Ÿæˆä¸­] ")
                
                response = client.chat.completions.create(
                    model=model,
                    max_tokens=self.max_tokens,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    stream=True
                )
                
                finish_reason = None
                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, 'content') and delta.content:
                            content += delta.content
                            printer.write(delta.content)
                        # Capture finish_reason from the last chunk
                        if chunk.choices[0].finish_reason:
                            finish_reason = chunk.choices[0].finish_reason
                
                printer.write("\n")
            
            # Print API response metadata
            print_current(f"ğŸ—œï¸ [LLM Summary] API Response - finish_reason: {finish_reason}")
            
            if not content:
                raise ValueError("Empty response from OpenAI API (streaming)")
            return content
        else:
            # Non-streaming mode
            response = client.chat.completions.create(
                model=model,
                max_tokens=self.max_tokens,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            if response and response.choices:
                finish_reason = response.choices[0].finish_reason
                usage = getattr(response, 'usage', None)
                if usage:
                    print_current(f"ğŸ—œï¸ [LLM Summary] API Response - finish_reason: {finish_reason}, prompt_tokens: {getattr(usage, 'prompt_tokens', 'N/A')}, completion_tokens: {getattr(usage, 'completion_tokens', 'N/A')}")
                else:
                    print_current(f"ğŸ—œï¸ [LLM Summary] API Response - finish_reason: {finish_reason}")
            
            if response and response.choices and response.choices[0].message:
                return response.choices[0].message.content
            raise ValueError("Empty response from OpenAI API")
    
    def _calculate_total_length(self, history: List[Dict[str, Any]]) -> int:
        """
        Calculate total character count of history records
        
        Args:
            history: List of history records
            
        Returns:
            Total character count
        """
        total = 0
        fields_to_count = ["prompt", "result", "content", "response", "output", "data"]
        for record in history:
            for field in fields_to_count:
                if field in record:
                    total += len(str(record[field]))
        return total
    
    def get_compression_stats(
        self,
        original_history: List[Dict[str, Any]],
        compressed_history: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """
        Get compression statistics
        
        Args:
            original_history: Original history
            compressed_history: Compressed history
            
        Returns:
            Compression stats
        """
        original_length = self._calculate_total_length(original_history)
        compressed_length = self._calculate_total_length(compressed_history)
        
        compression_ratio = (1 - compressed_length / original_length) * 100 if original_length > 0 else 0
        saved_chars = original_length - compressed_length
        estimated_token_savings = saved_chars // 4
        
        return {
            "compression_method": "llm_summary",
            "original_chars": original_length,
            "compressed_chars": compressed_length,
            "saved_chars": saved_chars,
            "compression_ratio": compression_ratio,
            "estimated_token_savings": estimated_token_savings,
            "original_records": len(original_history),
            "compressed_records": len(compressed_history),
        }
