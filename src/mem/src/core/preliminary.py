"""
Primary Memory Management Module
"""

import os
import time
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from functools import lru_cache
import pickle
import jieba
import re
import warnings

# 设置sklearn警告处理：将警告重定向到日志而不是终端
with warnings.catch_warnings():
    warnings.simplefilter("ignore", UserWarning)
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

from ..models.memory_cell import MemCell
from ..clients.llm_client import LLMClient
from ..clients.embedding_client import EmbeddingClient
from ..utils.config import ConfigLoader
from ..utils.logger import get_logger
from ..utils.exceptions import MemorySystemError
from ..models.mem import Mem
from ..utils.embedding_cache import get_global_cache_manager

logger = get_logger(__name__)


class PreliminaryMemoryManager:
    """
    Basic memory management module, responsible for summary+text, embedding, TF-IDF and other basic memory operations.
    """

    def __init__(self, storage_path: str = "memory/preliminary_memory", config_file: str = "config.txt", max_tokens: int = 4096):
        self.storage_path = storage_path
        os.makedirs(self.storage_path, exist_ok=True)
        self.max_tokens = max_tokens
        self.config_loader = ConfigLoader(config_file)
        self.similarity_threshold = self.config_loader.get_similarity_threshold()
        # Memory storage
        self.mem = Mem(
            storage_dir=self.storage_path,
            memory_name="preliminary"
        )
        self.llm_client = LLMClient(config_file=config_file)
        self.embedding_client = EmbeddingClient(config_file=config_file)
        # Embedding cache bound to current memory library directory
        from ..utils.embedding_cache import EmbeddingCacheManager
        self.embedding_cache = EmbeddingCacheManager(
            cache_path=os.path.join(self.storage_path, "embedding_cache")
        )

        # TF-IDF related initialization
        self.tfidf_cache_path = os.path.join(
            self.storage_path, "summary_tfidf_cache")
        os.makedirs(self.tfidf_cache_path, exist_ok=True)
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.memory_texts = []
        self.memory_ids = []
        self._init_tfidf()

    @staticmethod
    def _tokenize_chinese(text: str) -> List[str]:
        """Chinese word segmentation function"""
        return list(jieba.cut(text))

    def write_memory_auto(self, text: str) -> Dict[str, Any]:
        """Intelligently write memory, automatically determine whether to add or update."""
        try:
            if not text or not str(text).strip():
                return {
                    'success': False,
                    'error': 'Text is empty',
                    'action': 'error'
                }

            text_str = text if isinstance(text, str) else str(text)

            # Generate summary
            summary = self._generate_summary(text_str)

            # Find similar memories
            similar_memories = self._find_similar_memories(text_str)

            # Add debug information
            logger.info(f"Similarity threshold: {self.similarity_threshold}")
            if similar_memories:
                logger.info(f"Found {len(similar_memories)} similar memories")
                for i, mem in enumerate(similar_memories[:3]):  # Only show first 3
                    logger.info(
                        f"  Similar memory {i+1}: ID={mem['id']}, similarity={mem['similarity']:.3f}")
            else:
                logger.info("No similar memories found")

            if similar_memories and similar_memories[0]['similarity'] >= self.similarity_threshold:
                # Found similar memory and similarity reaches threshold, perform update
                best_match = similar_memories[0]
                mem_id = best_match['id']  # Use 'id' instead of 'mem_cell'
                logger.info(f"Similarity reaches threshold, updating memory: {mem_id}")

                # Use intelligent update method, consistent with legacy version
                updated_result = self._update_existing_memory_intelligently(
                    mem_id=mem_id,
                    new_text=text_str,
                    new_summary=summary
                )

                # Update TF-IDF model
                self._update_tfidf_model()

                return {
                    'success': True,
                    'action': 'updated',
                    'mem_id': mem_id,
                    'similarity_score': best_match['similarity'],
                    'mem_cell': updated_result.get('mem_cell'),
                    'versioned_text': updated_result.get('versioned_text', False),
                    'text_length': updated_result.get('text_length', 0),
                    'estimated_tokens': updated_result.get('estimated_tokens', 0),
                    'was_truncated': updated_result.get('was_truncated', False)
                }
            else:
                # No similar memory or similarity not enough, create new memory
                logger.info("Similarity not reaching threshold, creating new memory")
                new_mem = self.mem.add_memory(
                    text=text_str,
                    summary=summary
                )

                # Update TF-IDF model
                self._update_tfidf_model()

                return {
                    'success': True,
                    'action': 'added',
                    'mem_id': new_mem.mem_id,
                    'mem_cell': new_mem
                }

        except Exception as e:
            logger.error(f"Failed to write memory: {e}")
            return {
                'success': False,
                'error': str(e),
                'action': 'error'
            }

    def search_memories_by_query(self, query: str, top_k: int = 5, weights: Tuple[float, float] = (0.5, 0.5)) -> List[Dict[str, Any]]:
        """Search memories by content, supporting TF-IDF and embedding hybrid search."""
        try:
            if not query or not str(query).strip():
                logger.warning("Query is empty, returning empty results")
                return []

            mem_cells = self.mem.list_all()
            if not mem_cells:
                return []

            query_str = query if isinstance(query, str) else str(query)

            # Calculate embedding similarity
            query_embedding = self._create_embedding(query_str)
            embedding_similarities = {}

            for mem_cell in mem_cells:
                summary = mem_cell.summary
                if not isinstance(summary, str):
                    summary = str(summary)
                emb = self._create_embedding(summary)
                similarity = float(np.dot(query_embedding, emb))
                embedding_similarities[mem_cell.mem_id] = similarity

            # Calculate TF-IDF similarity
            tfidf_similarities = {}
            tfidf_results = self._calculate_tfidf_similarity(query_str)
            for mem_id, similarity in tfidf_results:
                tfidf_similarities[mem_id] = similarity

            # Combine similarity scores
            candidates = []
            embedding_weight, tfidf_weight = weights

            for mem_cell in mem_cells:
                mem_id = mem_cell.mem_id
                embedding_sim = embedding_similarities.get(mem_id, 0.0)
                tfidf_sim = tfidf_similarities.get(mem_id, 0.0)

                # Weighted combination
                combined_score = (embedding_weight * embedding_sim +
                                  tfidf_weight * tfidf_sim)

                candidates.append({
                    'mem_cell': mem_cell,
                    'similarity_score': combined_score,
                    'embedding_similarity': embedding_sim,
                    'tfidf_similarity': tfidf_sim
                })

            # Sort by combined score
            candidates.sort(key=lambda x: x['similarity_score'], reverse=True)

            # Take top_k results and increase recall count
            results = []
            for candidate in candidates[:top_k]:
                mem_cell = candidate['mem_cell']
                # Increase recall count using the memory manager's method
                self.mem.increment_recall(mem_cell.mem_id)
                # Update recall count in results
                candidate['recall_count'] = mem_cell.recall_cnt + 1
                results.append(candidate)

            return results

        except Exception as e:
            logger.error(f"Search memories failed: {e}")
            return []

    def search_memories_by_time(self, target_date: str, top_k: int = 5, sort_by: Optional[str] = None) -> List[Dict[str, Any]]:
        """Search memories by time."""
        try:
            time_info = self._parse_chinese_date(target_date)
            if time_info is None:
                logger.error(f"Could not parse date format: {target_date}")
                return []

            mem_cells = self.mem.list_all()
            if not mem_cells:
                return []

            target_memories = []
            for mem_cell in mem_cells:
                if self._is_in_time_range(mem_cell.create_time, time_info):
                    result_dict = {
                        "mem_cell": mem_cell,
                        "match_type": "create_time",
                        "time_query_type": time_info["type"],
                        "recall_count": mem_cell.recall_cnt,
                        "update_time": mem_cell.update_time,
                        "create_time": mem_cell.create_time
                    }
                    target_memories.append(result_dict)
                elif (mem_cell.update_time != mem_cell.create_time and
                      self._is_in_time_range(mem_cell.update_time, time_info)):
                    result_dict = {
                        "mem_cell": mem_cell,
                        "match_type": "update_time",
                        "time_query_type": time_info["type"],
                        "recall_count": mem_cell.recall_cnt,
                        "update_time": mem_cell.update_time,
                        "create_time": mem_cell.create_time
                    }
                    target_memories.append(result_dict)

            # Sort
            if sort_by == "recall_count":
                target_memories.sort(
                    key=lambda x: x["recall_count"], reverse=True)
            elif sort_by == "update_time":
                target_memories.sort(
                    key=lambda x: x["update_time"], reverse=True)
            elif sort_by == "create_time":
                target_memories.sort(
                    key=lambda x: x["create_time"], reverse=True)
            else:
                # Default sort by recall count
                target_memories.sort(
                    key=lambda x: x["recall_count"], reverse=True)

            query_type_desc = {
                "year_only": f"{time_info['year']}",
                "year_month": f"{time_info['year']}-{time_info.get('month', '')}",
                "full_date": f"{time_info['year']}-{time_info.get('month', '')}-{time_info.get('day', '')}"
            }.get(time_info["type"], target_date)

            logger.info(f"Found {len(target_memories)} memories for {query_type_desc}")

            # Take top_k results and increase recall count
            final_results = target_memories[:top_k]
            for result in final_results:
                mem_cell = result['mem_cell']
                # Increase recall count
                self.mem.increment_recall(mem_cell.mem_id)
                # Update recall count in results
                result['recall_count'] = mem_cell.recall_cnt + 1

            return final_results

        except Exception as e:
            logger.error(f"Search memories by time failed: {e}")
            return []

    def _parse_chinese_date(self, date_str: str) -> Optional[Dict[str, Any]]:
        """
        Parse Chinese date string.
        Args:
            date_str (str): Date string.
        Returns:
            Optional[Dict[str, Any]]: Parsed result.
        """
        try:
            date_str = date_str.strip().replace(' ', '').replace('\n', '')

            # Match "2024年" format
            year_match = re.match(r'^(\d{4})年$', date_str)
            if year_match:
                return {
                    "type": "year_only",
                    "year": int(year_match.group(1))
                }

            # Match "2024年3月" format
            year_month_match = re.match(r'^(\d{4})年(\d{1,2})月$', date_str)
            if year_month_match:
                return {
                    "type": "year_month",
                    "year": int(year_month_match.group(1)),
                    "month": int(year_month_match.group(2))
                }

            # Match "2024年3月15日" format
            full_date_match = re.match(
                r'^(\d{4})年(\d{1,2})月(\d{1,2})日$', date_str)
            if full_date_match:
                return {
                    "type": "full_date",
                    "year": int(full_date_match.group(1)),
                    "month": int(full_date_match.group(2)),
                    "day": int(full_date_match.group(3))
                }

            # Handle fuzzy time expressions as fallback
            now = time.localtime()
            
            if "今天" in date_str:
                return {
                    "type": "full_date",
                    "year": now.tm_year,
                    "month": now.tm_mon,
                    "day": now.tm_mday
                }
            elif "昨天" in date_str:
                yesterday = time.localtime(time.time() - 24*60*60)
                return {
                    "type": "full_date",
                    "year": yesterday.tm_year,
                    "month": yesterday.tm_mon,
                    "day": yesterday.tm_mday
                }
            elif "明天" in date_str:
                tomorrow = time.localtime(time.time() + 24*60*60)
                return {
                    "type": "full_date",
                    "year": tomorrow.tm_year,
                    "month": tomorrow.tm_mon,
                    "day": tomorrow.tm_mday
                }
            elif "这个月" in date_str or "本月" in date_str:
                return {
                    "type": "year_month",
                    "year": now.tm_year,
                    "month": now.tm_mon
                }
            elif "上个月" in date_str:
                if now.tm_mon == 1:
                    return {
                        "type": "year_month",
                        "year": now.tm_year - 1,
                        "month": 12
                    }
                else:
                    return {
                        "type": "year_month",
                        "year": now.tm_year,
                        "month": now.tm_mon - 1
                    }
            elif "今年" in date_str:
                return {
                    "type": "year_only",
                    "year": now.tm_year
                }
            elif "去年" in date_str:
                return {
                    "type": "year_only",
                    "year": now.tm_year - 1
                }

            return None
        except Exception as e:
            logger.error(f"Failed to parse Chinese date: {e}")
            return None

    def _is_in_time_range(self, timestamp: float, time_info: Dict[str, Any]) -> bool:
        """
        Check if timestamp is within the specified time range.
        Args:
            timestamp (float): Timestamp.
            time_info (Dict[str, Any]): Time range information.
        Returns:
            bool: Whether it is within the range.
        """
        try:
            target_time = time.localtime(timestamp)
            if time_info["type"] == "year_only":
                return target_time.tm_year == time_info["year"]
            elif time_info["type"] == "year_month":
                return (target_time.tm_year == time_info["year"] and
                        target_time.tm_mon == time_info["month"])
            elif time_info["type"] == "full_date":
                return (target_time.tm_year == time_info["year"] and
                        target_time.tm_mon == time_info["month"] and
                        target_time.tm_mday == time_info["day"])
            return False
        except Exception as e:
            logger.error(f"Failed to check time range: {e}")
            return False

    def add_memory_with_timestamp(self, text: str, create_time: float, summary: str = None) -> Dict[str, Any]:
        """Add memory and specify creation time."""
        try:
            if summary is None:
                summary = self._generate_summary(text)
            mem_id = f"mem_{int(create_time * 1000)}"

            # Create text file path
            text_file_path = os.path.join(self.mem.text_dir, f"{mem_id}.md")

            mem_cell = MemCell(
                text_file_path=text_file_path,
                summary=summary,
                create_time=create_time,
                update_time=create_time,
                mem_id=mem_id
            )
            # Set text content (will be saved to file automatically)
            mem_cell.text = [text]

            self.mem.mem_cells.append(mem_cell)
            self.mem.mem_id_index[mem_cell.mem_id] = len(
                self.mem.mem_cells) - 1

            # Save index file
            self.mem._save()

            return {
                'action': 'create',
                'mem_id': mem_id,
                'summary': summary,
                'operation': 'created'
            }
        except Exception as e:
            logger.error(f"Failed to add memory with timestamp: {e}")
            return {
                'action': 'error',
                'error': str(e),
                'operation': 'error'
            }

    def get_memory_stats(self) -> Dict[str, Any]:
        """Get memory statistics."""
        try:
            count = len(self.mem.mem_cells)
            return {
                'memory_count': count,
                'storage_size_mb': 0.0  # TODO: Calculate storage size
            }
        except Exception as e:
            logger.error(f"Failed to get memory statistics: {e}")
            return {
                'memory_count': 0,
                'storage_size_mb': 0.0
            }

    # --- TF-IDF related methods ---
    def _init_tfidf(self):
        """Initialize TF-IDF model"""
        try:
            tfidf_model_path = os.path.join(
                self.tfidf_cache_path, "tfidf_model.pkl")
            if os.path.exists(tfidf_model_path):
                with open(tfidf_model_path, 'rb') as f:
                    data = pickle.load(f)
                    self.tfidf_vectorizer = data['vectorizer']
                    self.tfidf_matrix = data['matrix']
                    self.memory_texts = data['texts']
                    self.memory_ids = data['ids']
                logger.info(f"Loaded TF-IDF model, containing {len(self.memory_ids)} documents")
            else:
                self._build_tfidf_model()
        except Exception as e:
            logger.error(f"Failed to initialize TF-IDF: {e}")
            self._build_tfidf_model()

    def _build_tfidf_model(self):
        """Build TF-IDF model"""
        try:
            mem_cells = self.mem.list_all()
            if not mem_cells:
                self.tfidf_vectorizer = None
                self.tfidf_matrix = None
                self.memory_texts = []
                self.memory_ids = []
                return

            # Prepare text data
            texts = []
            ids = []
            for mem_cell in mem_cells:
                # Use combined summary and original text
                combined_text = f"{mem_cell.summary} {' '.join(mem_cell.text)}"
                texts.append(combined_text)
                ids.append(mem_cell.mem_id)

            # Create TF-IDF vectorizer, using class-level word segmentation function
            self.tfidf_vectorizer = TfidfVectorizer(
                tokenizer=self._tokenize_chinese,
                token_pattern=None,  # 显式设置为None以避免警告
                max_features=10000,
                stop_words=None,
                ngram_range=(1, 2)
            )

            # Build TF-IDF matrix
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
            self.memory_texts = texts
            self.memory_ids = ids

            # Save model
            self._save_tfidf_model()
            logger.info(f"TF-IDF model built, containing {len(ids)} documents")
        except Exception as e:
            logger.error(f"Failed to build TF-IDF model: {e}")

    def _save_tfidf_model(self):
        """Save TF-IDF model"""
        try:
            if self.tfidf_vectorizer is not None:
                data = {
                    'vectorizer': self.tfidf_vectorizer,
                    'matrix': self.tfidf_matrix,
                    'texts': self.memory_texts,
                    'ids': self.memory_ids
                }
                tfidf_model_path = os.path.join(
                    self.tfidf_cache_path, "tfidf_model.pkl")
                with open(tfidf_model_path, 'wb') as f:
                    pickle.dump(data, f)
                logger.info("TF-IDF model saved successfully")
        except Exception as e:
            logger.error(f"Failed to save TF-IDF model: {e}")

    def _update_tfidf_model(self):
        """Update TF-IDF model"""
        self._build_tfidf_model()

    @lru_cache(maxsize=64)
    def _calculate_tfidf_similarity(self, query: str) -> List[Tuple[str, float]]:
        """Calculate TF-IDF similarity"""
        try:
            if self.tfidf_vectorizer is None or self.tfidf_matrix is None:
                logger.warning("TF-IDF model not initialized")
                return []

            # Vectorize query text
            query_vector = self.tfidf_vectorizer.transform([query])

            # Calculate similarity
            similarities = cosine_similarity(
                query_vector, self.tfidf_matrix).flatten()

            # Return results
            results = []
            for i, similarity in enumerate(similarities):
                if similarity > 0:
                    results.append((self.memory_ids[i], float(similarity)))

            # Sort by similarity
            results.sort(key=lambda x: x[1], reverse=True)
            return results
        except Exception as e:
            logger.error(f"Failed to calculate TF-IDF similarity: {e}")
            return []

    def update_tfidf_model(self) -> Dict[str, Any]:
        """Manually update TF-IDF model"""
        try:
            self._update_tfidf_model()
            return {
                'success': True,
                'message': 'TF-IDF model updated successfully',
                'document_count': len(self.memory_ids)
            }
        except Exception as e:
            logger.error(f"Failed to update TF-IDF model: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def get_tfidf_stats(self) -> Dict[str, Any]:
        """Get TF-IDF model statistics"""
        try:
            if self.tfidf_vectorizer is None:
                return {
                    'model_initialized': False,
                    'document_count': 0,
                    'feature_count': 0
                }

            return {
                'model_initialized': True,
                'document_count': len(self.memory_ids),
                'feature_count': self.tfidf_vectorizer.get_feature_names_out().shape[0],
                'matrix_shape': self.tfidf_matrix.shape if self.tfidf_matrix is not None else None
            }
        except Exception as e:
            logger.error(f"Failed to get TF-IDF statistics: {e}")
            return {
                'model_initialized': False,
                'error': str(e)
            }

    # --- Internal methods ---
    def _generate_summary(self, text: str) -> str:
        """Generate summary (call LLM)"""
        try:
            response = self.llm_client.generate_response(
                prompt=text,
                system_prompt="Please generate a concise summary for the following content.",
                max_tokens=128,
                stream=False  # Force non-streaming response
            )

            # Ensure return is string
            if isinstance(response, dict) and response.get("stream"):
                # If streaming response, extract content
                return self.llm_client.get_stream_content(response)
            elif isinstance(response, str):
                return response
            else:
                # Other cases, convert to string
                return str(response)

        except Exception as e:
            logger.warning(f"Failed to generate summary: {e}")
            return text[:50]

    @lru_cache(maxsize=64)
    def _create_embedding(self, text: str) -> np.ndarray:
        """Create text embedding, prioritize cache, ensure text is str type"""
        if not isinstance(text, str):
            text = str(text)
        # Prioritize cache
        cached = self.embedding_cache.get_cached_embedding(text)
        if cached is not None:
            return cached
        emb = self.embedding_client.create_embedding(text)
        self.embedding_cache.cache_embedding(text, emb)
        return emb

    def _find_similar_memories(self, text: str) -> List[Dict[str, Any]]:
        """Find similar memories (based on embedding)"""
        if not isinstance(text, str):
            text = str(text)
        query_emb = self._create_embedding(text)
        mem_cells = self.mem.list_all()
        results = []
        for mem_cell in mem_cells:
            summary = mem_cell.summary
            if not isinstance(summary, str):
                summary = str(summary)
            emb = self._create_embedding(summary)
            similarity = float(np.dot(query_emb, emb))
            results.append({'id': mem_cell.mem_id, 'similarity': similarity})
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results

    def _update_existing_memory_intelligently(self, mem_id: str, new_text: str, new_summary: str) -> Dict[str, Any]:
        """Intelligent update of existing memory, consistent with legacy version"""
        try:
            existing_mem = self.mem.get(mem_id)
            if not existing_mem:
                return {
                    "action": "error",
                    "error": f"Memory not found: {mem_id}",
                    "mem_id": mem_id,
                    "operation": "update_failed"
                }

            # Build versioned text
            versioned_text = self._build_versioned_text(existing_mem, new_text)

            # Check token limit
            current_tokens = self._estimate_tokens(versioned_text)
            if current_tokens > self.max_tokens:
                logger.info(
                    f"Text exceeds token limit ({current_tokens} > {self.max_tokens}), truncating")
                versioned_text = self._truncate_text_to_tokens(
                    versioned_text, self.max_tokens)
                truncated_tokens = self._estimate_tokens(versioned_text)
                logger.info(f"Token count after truncation: {truncated_tokens}")

            # Generate new summary for versioned text
            updated_summary = self._generate_summary(versioned_text)

            # Update memory
            updated_mem = self.mem.update_memory(
                mem_id=mem_id,
                new_text=versioned_text,
                new_summary=updated_summary
            )

            logger.info(f"Memory updated successfully: {mem_id}")
            logger.info(f"Versioned text length: {len(versioned_text)} characters")
            logger.info(f"Estimated token count: {self._estimate_tokens(versioned_text)}")

            return {
                "action": "update",
                "mem_id": mem_id,
                "summary": updated_summary,
                "operation": "memory_updated",
                "original_summary": existing_mem.summary,
                "new_content_added": new_text,
                "text_replaced": True,
                "summary_merged": True,
                "versioned_text": True,
                "text_length": len(versioned_text),
                "estimated_tokens": self._estimate_tokens(versioned_text),
                "was_truncated": current_tokens > self.max_tokens,
                "mem_cell": updated_mem
            }

        except Exception as e:
            logger.error(f"Failed to update memory intelligently: {e}")
            return {
                "action": "error",
                "error": str(e),
                "mem_id": mem_id,
                "operation": "update_failed"
            }

    def _build_versioned_text(self, existing_mem: MemCell, new_text: str) -> str:
        """
        Build versioned text content, consistent with legacy version.
        Args:
            existing_mem (MemCell): Existing memory cell.
            new_text (str): New text.
        Returns:
            str: Versioned text.
        """
        try:
            existing_text = " ".join(
                existing_mem.text) if existing_mem.text else ""
            current_version = existing_mem.update_cnt + 1

            if existing_text:
                if existing_text.startswith("Version"):
                    version_match = re.match(r'Version(\d+)：', existing_text)
                    if version_match:
                        existing_version = int(version_match.group(1))
                        content_start = existing_text.find("：", 2) + 1
                        existing_content = existing_text[content_start:].strip(
                        )
                        versioned_text = f"Version{current_version}：{new_text}\nVersion{existing_version}：{existing_content}"
                        logger.info(
                            f"Updated from version {existing_version} to version {current_version}")
                    else:
                        versioned_text = f"Version{current_version}：{new_text}\nVersion0：{existing_text}"
                        logger.info(f"Could not parse version number, treated as version 0, new version is {current_version}")
                else:
                    versioned_text = f"Version{current_version}：{new_text}\nVersion0：{existing_text}"
                    logger.info(f"Non-versioned text, treated as version 0, new version is {current_version}")
            else:
                versioned_text = f"Version{current_version}：{new_text}"
                logger.info(f"No existing text, directly as version {current_version}")

            logger.info(f"Building versioned text, version number: {current_version}")
            logger.info(f"Existing memory update count: {existing_mem.update_cnt}")
            return versioned_text

        except Exception as e:
            logger.error(f"Failed to build versioned text: {e}")
            existing_text = " ".join(
                existing_mem.text) if existing_mem.text else ""
            return f"{new_text}\n{existing_text}".strip()

    def _create_new_memory(self, text: str, summary: str) -> Dict[str, Any]:
        """Create new memory"""
        create_time = time.time()
        mem_id = f"mem_{int(create_time * 1000)}"
        mem_cell = MemCell(
            text_file_path=os.path.join(self.mem.text_dir, f"{mem_id}.md"),
            summary=summary,
            create_time=create_time,
            update_time=create_time,
            mem_id=mem_id
        )
        mem_cell.text = [text]
        self.mem.mem_cells.append(mem_cell)
        self.mem.mem_id_index[mem_cell.mem_id] = len(self.mem.mem_cells) - 1
        return {
            'action': 'create',
            'mem_id': mem_id,
            'summary': summary,
            'operation': 'created'
        }

    def _estimate_tokens(self, text: str) -> int:
        """
        Roughly estimate the number of tokens in the text, consistent with legacy version.
        Args:
            text (str): Input text.
        Returns:
            int: Estimated token count.
        """
        if not text:
            return 0
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'[a-zA-Z]+', text))
        other_chars = len(text) - chinese_chars - english_words
        estimated_tokens = chinese_chars // 2 + english_words + other_chars // 4
        return max(1, estimated_tokens)

    def _truncate_text_to_tokens(self, text: str, max_tokens: int) -> str:
        """
        Truncate text to the maximum number of tokens, consistent with legacy version.
        Args:
            text (str): Input text.
            max_tokens (int): Maximum number of tokens.
        Returns:
            str: Truncated text.
        """
        if not text:
            return text
        current_tokens = self._estimate_tokens(text)
        if current_tokens <= max_tokens:
            return text
        left, right = 0, len(text)
        best_length = 0
        while left <= right:
            mid = (left + right) // 2
            truncated_text = text[:mid]
            tokens = self._estimate_tokens(truncated_text)
            if tokens <= max_tokens:
                best_length = mid
                left = mid + 1
            else:
                right = mid - 1
        return text[:best_length]
