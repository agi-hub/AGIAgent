# Language setting: en for English, zh for Chinese
#LANG=en
LANG=zh

# GUI API configuration
api_key=your key
api_base=https://open.bigmodel.cn/api/anthropic
model=glm-4.6
max_tokens=16384

# OpenAI API configuration 
# api_key=your key
# api_base=https://pmpjfbhq.cn-nb1.rainapp.top/v1
# model=gpt-4.1
# max_tokens=8192

# Anthropic models
# api_key=your key
# api_base=https://api.openai-proxy.org/anthropic
# model=claude-sonnet-4-0
# max_tokens=32768

# Openrouter
# api_key=your key
# api_base=https://openrouter.ai/api/v1
# model=qwen/qwen3-30b-a3b-instruct-2507
# max_tokens=32768

# Zhipu API configuration 
# api_key=your key
# api_base=https://open.bigmodel.cn/api/anthropic
# model=glm-4.5
# max_tokens=16384

# Bailian API configuration 
# api_key=your key
# api_base=https://dashscope.aliyuncs.com/compatible-mode/v1
# model=qwen3-coder-480b-a35b-instruct
# max_tokens=65536

# Google gemini model
# api_key=your key
# api_base=https://api.openai-proxy.org/v1
# model=gemini-2.5-pro
# max_tokens=8192

# DeepSeek API configuration
# api_key=your key
# api_base=https://api.deepseek.com/v1
# model=deepseek-chat

# Volcengine Doubao API configuration
# api_key=your key
# api_base=https://ark.cn-beijing.volces.com/api/v3
# model=doubao-1-5-pro-32k-250115

# Moonshot
# api_key=your key
# api_base=https://api.moonshot.cn/anthropic
# model=kimi-k2-0711-preview
# max_tokens=8192

# SiliconFlow API configuration 
# api_key=your key
# api_base=https://api.siliconflow.cn/v1
# model=Qwen/Qwen3-30B-A3B
# max_tokens=4096

# Ollama (local serve)
# api_key=your key
# api_base=http://localhost:11434/v1
# model=qwen3:8b
# max_tokens=4096

# Note: only the last configuration is valid.

# Streaming output configuration: True for streaming, False for batch output
streaming=True

# Long-term memory configuration
enable_long_term_memory=False

# Long-term memory model configuration (default to the same as model and api_base)
mem_model=gpt-4.1
mem_model_api_key=your key
mem_model_api_base=https://api.openai-proxy.org/v1

# Long-term memory embedding model, if using long-term memory, embedding_model is required.
embedding_model=BAAI/bge-m3
embedding_model_api_key=your key
embedding_model_api_base=https://api.siliconflow.cn/v1

# Vision model configuration (for image recognition with get_sensor_data)
# If not configured, will use the main model (model, api_key, api_base) for vision tasks
# It's recommended to configure a vision-capable model here (e.g., GPT-4 Vision, Claude 3, Gemini)
# vision_model=gpt-4o
# vision_api_key=your key
# vision_api_base=https://api.openai.com/v1
# vision_max_tokens=4096

# Tool calling format configuration: True for standard tool calling, False for chat-based tool calling
# When set to True, uses native tool calling API (for models that support it like GPT-4, Claude)
# When set to False, uses chat-based tool calling (tools described in messages)
# This overrides the automatic model-based detection
# Default: True (use standard tool calling when available)
Tool_calling_format=False

# Truncation Length Configuration
# These configurations control the length of information returned by tool calls to the large model,
# preventing overly long content from affecting performance

# Main tool result truncation length, default 10000 characters
# Used for truncating tool execution results, parameter displays, formatted outputs, etc.

# Debug system configuration
# Enable debug system for enhanced debugging capabilities
# When set to True, enables stack trace, memory monitoring, and execution tracking
enable_debug_system=False

# Recommended values: 5000-20000, adjust based on model context window size
# Affects:
# - Tool results returned to LLM
# - Tool parameter displays
# - Search result summaries
# - Output limits in debug logs
truncation_length=20000

# Web content truncation length, default 50000 characters, uses 5x truncation_length if not set
# Used for truncating large-capacity content like web search results
# Recommended values: 10000-100000
# Affects:
# - Web search content
# - Web scraping results
# - Large document content
web_content_truncation_length=20000

# Usage recommendations:
# 1. If using models with smaller context windows (<32K), consider lowering all truncation values
# 2. If using models with large context windows (>128K), you can appropriately increase truncation values
# 3. When processing large amounts of data, you can temporarily increase web_content_truncation_length
# 4. Changes to configuration require program restart to take effect

# Minimum content length to trigger compression (in characters)
# Content shorter than this will not be compressed
# Lower values compress more aggressively, higher values preserve more content
# Recommended values: 200-1000 characters
compression_min_length=500

# Number of characters to keep at the beginning of compressed content
# Lower values compress more aggressively, higher values preserve more context
# Recommended values: 50-200 characters
compression_head_length=100

# Number of characters to keep at the end of compressed content
# Lower values compress more aggressively, higher values preserve more context
# Recommended values: 50-200 characters
compression_tail_length=100

# Simplified search result terminal output (default: True)
# When enabled, workspace_search and web_search only display simplified result summaries in terminal
# When disabled, display full search result details
simplified_search_output=True

# Summary report generation (default: False)
# When enabled, generates single task summary and task summary reports
# When disabled, skips summary report generation to save time and resources
# Affects:
# - Single task summary generation
# - Task summary report creation
# - Summary markdown file output
summary_report=False

# Web Search Summary Configuration
# Controls whether to use AI to generate comprehensive summaries of web search results
# When enabled, the search results will be analyzed and summarized by the large model
# providing detailed individual webpage analysis and synthesis across all sources
# Default: True (enabled for better user experience)
web_search_summary=False

# Web search summary usage recommendations:
# 1. Enable web_search_summary=True for comprehensive analysis of search results
# 2. Disable web_search_summary=False to get only individual webpage content without AI summary
# 3. When enabled, requires valid API configuration and LLM model for summarization
# 4. Summarization focuses on extracting information relevant to the search query
# 5. Each webpage result is analyzed individually with file location references
# 6. Changes to this setting take effect immediately for new searches

# GUI Default User Data Directory Configuration
# Specifies the default directory for GUI to display file lists and manage workspace directories
# If not set or the specified directory doesn't exist, uses the current working directory as default
# This directory should contain subdirectories with 'workspace' folders for proper GUI functionality
gui_default_data_directory=../data

# Interactive Command Auto-Fix Configuration
# Controls whether to automatically fix interactive commands to non-interactive versions
# When enabled, the system will automatically add flags like --quiet, -y, -n to commands
# to prevent them from requiring user input during execution
# Default: False (disabled)
# Set to True to enable automatic modification of interactive commands
auto_fix_interactive_commands=True

# Multi-Agent Mode Configuration
# Controls whether to enable multi-agent functionality
# When enabled, multi-agent tools like spawn_agent, wait_for_agibot_spawns, etc. are available
# When disabled, these tools are hidden from the model to reduce complexity
# Default: True (enabled)
# Set to False to disable multi-agent capabilities
multi_agent=False

# Round Synchronization Barrier (experimental)
# When enabled, all agents will execute in lockstep windows.
# After each agent executes N rounds (sync_round), it will enter wait_for_sync state.
# A separate sync manager monitors agents' status files, and when ALL are waiting,
# it releases a sync signal so each agent can proceed to next N rounds.
# Set enable_round_sync to true to enable; sync_round must be >= 1 (default: 2)
enable_round_sync=True
sync_round=2

# Jieba Chinese Segmentation Configuration
# Controls whether to enable jieba Chinese text segmentation for code parsing
# When enabled, jieba will be used to segment Chinese comments and text for better search results
# When disabled, only basic English tokenization is used (recommended for better performance)
# Default: False (disabled)
# Set to True to enable jieba Chinese segmentation
enable_jieba=True

# Emoji Display Configuration
# Controls whether to display emoji symbols in print outputs
# When enabled (False), emoji symbols are displayed normally
# When disabled (True), emoji symbols are filtered out using regex before printing
# This only removes emoji characters, preserving other Unicode text (Chinese, etc.)
# This is useful for systems that don't support emoji fonts properly
# Default: False (emoji enabled)
# Set to True to disable emoji display
emoji_disabled=False

# Markdown file auto-conversion configuration
# Controls whether to automatically convert markdown files to Word, PDF, and LaTeX formats after editing
# When enabled (True), the corresponding format will be automatically generated when a markdown file is edited
# When disabled (False), the format will not be automatically generated
# Default: True for Word and PDF (backward compatibility), False for LaTeX
auto_convert_to_word=True
auto_convert_to_pdf=True
auto_convert_to_latex=False

