# Language setting: en for English, zh for Chinese
# LANG=en
LANG=zh

# GUI API configuration 
api_key=your key
api_base=https://api.openai-proxy.org/v1


# OpenAI API configuration 
# api_key=your key
# api_base=https://api.openai-proxy.org/v1
# model=gpt-4.1
# max_tokens=8192

# Bailian API configuration 
# api_key=your key
# api_base=https://dashscope.aliyuncs.com/compatible-mode/v1
# model=qwen3-coder-480b-a35b-instruct
# model=qwen3-coder-30b-a3b-instruct
# max_tokens=65536

# model=qwen3-235b-a22b-instruct-2507
# max_tokens=32768

# model=qwen3-coder-plus
# max_tokens=32768

# Anthropic models
api_key=your key
api_base=https://api.openai-proxy.org/anthropic
model=claude-sonnet-4-0
max_tokens=16384

# Google gemini model
# api_key=your key
# api_base=https://api.openai-proxy.org/v1
# model=gemini-2.5-pro
# max_tokens=8192

# DeepSeek API configuration
# api_key=your key
# api_base=https://api.deepseek.com/v1
# model=deepseek-chat

# Volcengine Doubao API configuration
# api_key=your key
# api_base=https://ark.cn-beijing.volces.com/api/v3
# model=doubao-1-5-pro-32k-250115

# Moonshot
# api_key=your key
# api_base=https://api.moonshot.cn/anthropic
# model=kimi-k2-0711-preview
# max_tokens=8192

# SiliconFlow API configuration 
# api_key=your key
# api_base=https://api.siliconflow.cn/v1
# model=Qwen/Qwen3-8B
# model=Qwen/Qwen3-30B-A3B
# model=Qwen/Qwen2.5-7B-Instruct
# max_tokens=4096

# Ollama (local serve)
# api_key=your key
# api_base=http://localhost:11434/v1
# model=qwen3:8b
# max_tokens=4096


# Streaming output configuration: True for streaming, False for batch output
streaming=True

# Long-term memory configuration
enable_long_term_memory=True

# Long-term memory model configuration (default to the same as model and api_base)
mem_model=gpt-4.1
mem_model_api_key=your key
mem_model_api_base=https://api.openai-proxy.org/v1

# Long-term memory embedding model, if using long-term memory, embedding_model is required.
embedding_model=BAAI/bge-m3
embedding_model_api_key=your key
embedding_model_api_base=https://api.siliconflow.cn/v1

# Tool calling format configuration: True for standard tool calling, False for chat-based tool calling
# When set to True, uses native tool calling API (for models that support it like GPT-4, Claude)
# When set to False, uses chat-based tool calling (tools described in messages)
# This overrides the automatic model-based detection
# Default: True (use standard tool calling when available)
Tool_calling_format=True

# Truncation Length Configuration
# These configurations control the length of information returned by tool calls to the large model,
# preventing overly long content from affecting performance

# Main tool result truncation length, default 10000 characters
# Used for truncating tool execution results, parameter displays, formatted outputs, etc.
# Recommended values: 5000-20000, adjust based on model context window size
# Affects:
# - Tool results returned to LLM
# - Tool parameter displays
# - Search result summaries
# - Output limits in debug logs
truncation_length=20000

# Web content truncation length, default 50000 characters, uses 5x truncation_length if not set
# Used for truncating large-capacity content like web search results
# Recommended values: 10000-100000
# Affects:
# - Web search content
# - Web scraping results
# - Large document content
web_content_truncation_length=20000

# Usage recommendations:
# 1. If using models with smaller context windows (<32K), consider lowering all truncation values
# 2. If using models with large context windows (>128K), you can appropriately increase truncation values
# 3. When processing large amounts of data, you can temporarily increase web_content_truncation_length
# 4. Changes to configuration require program restart to take effect

# History Summarization Configuration
# Controls whether to use AI to summarize conversation history to reduce context length
# When enabled, conversation history is summarized using the large model when it exceeds trigger length
# This replaces truncation with intelligent summarization to maintain important context
summary_history=True

# Maximum length for conversation history summary (in characters)
# Used when summary_history=True to control the length of the generated summary
# Recommended values: 3000-8000 characters
# Lower values save more context but may lose some detail
# Higher values preserve more information but use more context
summary_max_length=5000

# Trigger length for conversation history summarization (in characters)
# Only when the total conversation history exceeds this length will summarization be triggered
# Recommended values: 50000-120000 characters
# Lower values trigger summarization earlier, saving more context
# Higher values delay summarization, preserving more detailed conversation history
summary_trigger_length=100000

# History summarization usage recommendations:
# 1. Enable summary_history=True for long conversation sessions to control context length
# 2. Adjust summary_trigger_length based on when you want summarization to start (100000 is good for most cases)
# 3. Adjust summary_max_length based on your model's context window and task complexity
# 4. For critical tasks requiring full context, keep summary_history=False
# 5. When enabled, the latest tool result is always preserved in full text
# 6. Summarization only occurs when conversation exceeds trigger length, not every round
# 7. Summary generation uses the same model configured above, ensure it has good summarization capabilities

# Simplified search result terminal output (default: True)
# When enabled, codebase_search and web_search only display simplified result summaries in terminal
# When disabled, display full search result details
simplified_search_output=True

# Summary report generation (default: False)
# When enabled, generates single task summary and task summary reports
# When disabled, skips summary report generation to save time and resources
# Affects:
# - Single task summary generation
# - Task summary report creation
# - Summary markdown file output
summary_report=False

# Web Search Summary Configuration
# Controls whether to use AI to generate comprehensive summaries of web search results
# When enabled, the search results will be analyzed and summarized by the large model
# providing detailed individual webpage analysis and synthesis across all sources
# Default: True (enabled for better user experience)
web_search_summary=False

# Web search summary usage recommendations:
# 1. Enable web_search_summary=True for comprehensive analysis of search results
# 2. Disable web_search_summary=False to get only individual webpage content without AI summary
# 3. When enabled, requires valid API configuration and LLM model for summarization
# 4. Summarization focuses on extracting information relevant to the search query
# 5. Each webpage result is analyzed individually with file location references
# 6. Changes to this setting take effect immediately for new searches

# GUI Default User Data Directory Configuration
# Specifies the default directory for GUI to display file lists and manage workspace directories
# If not set or the specified directory doesn't exist, uses the current working directory as default
# This directory should contain subdirectories with 'workspace' folders for proper GUI functionality
gui_default_data_directory=.

# Interactive Command Auto-Fix Configuration
# Controls whether to automatically fix interactive commands to non-interactive versions
# When enabled, the system will automatically add flags like --quiet, -y, -n to commands
# to prevent them from requiring user input during execution
# Default: False (disabled)
# Set to True to enable automatic modification of interactive commands
auto_fix_interactive_commands=True

# Multi-Agent Mode Configuration
# Controls whether to enable multi-agent functionality
# When enabled, multi-agent tools like spawn_agibot, wait_for_agibot_spawns, etc. are available
# When disabled, these tools are hidden from the model to reduce complexity
# Default: True (enabled)
# Set to False to disable multi-agent capabilities
multi_agent=False

# Jieba Chinese Segmentation Configuration
# Controls whether to enable jieba Chinese text segmentation for code parsing
# When enabled, jieba will be used to segment Chinese comments and text for better search results
# When disabled, only basic English tokenization is used (recommended for better performance)
# Default: False (disabled)
# Set to True to enable jieba Chinese segmentation
enable_jieba=True
