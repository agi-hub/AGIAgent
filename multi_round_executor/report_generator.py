#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Copyright (c) 2025 AGI Bot Research Group.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

"""
Report generator for creating execution reports and summary reports
"""

# Application name macro definition
APP_NAME = "AGI Bot"

import os
import json
from datetime import datetime
from typing import Dict, Any, List

# å¯¼å…¥config_loaderä»¥èŽ·å–æˆªæ–­é•¿åº¦é…ç½®
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config_loader import get_truncation_length, get_summary_report

from .config import REPORT_TIMESTAMP_FORMAT, DATETIME_FORMAT, read_language_config
from tools.print_system import print_system, print_current


class ReportGenerator:
    """Report generator for creating detailed execution and summary reports"""
    
    def __init__(self, executor, logs_dir: str, workspace_dir: str = None):
        """
        Initialize report generator
        
        Args:
            executor: Tool executor instance
            logs_dir: Logs directory path
            workspace_dir: Workspace directory path
        """
        self.executor = executor
        self.logs_dir = logs_dir
        self.workspace_dir = workspace_dir
    
    def generate_execution_summary(self, report: Dict[str, Any]) -> str:
        """
        Generate execution summary
        
        Args:
            report: Execution report
            
        Returns:
            Execution summary text
        """
        start_time = datetime.fromisoformat(report["start_time"])
        end_time = datetime.fromisoformat(report["end_time"])
        duration = end_time - start_time
        
        # Check if it's the new todo file processing mode
        if "todo_file" in report:
            # New mode: process entire todo file
            todo_file = report.get("todo_file", "Unknown file")
            success = report.get("success", False)
            task_history = report.get("task_history", [])
            rounds_executed = len([h for h in task_history if isinstance(h, dict) and "round" in h])
            
            summary = f"""
Task execution summary:
- Processed file: {todo_file}
- Execution status: {'Success' if success else 'Failed'}
- Execution rounds: {rounds_executed}/{report.get("subtask_loops", 0)}
- Total duration: {duration}
- Workspace directory: {report.get("workspace_dir", "Not specified")}
"""
            
            if not success and "error" in report:
                summary += f"- Error message: {report['error']}\n"
                
        else:
            # Legacy mode: compatibility support
            total_tasks = report.get("total_tasks", 0)
            completed_count = len(report.get("completed_tasks", []))
            failed_count = len(report.get("failed_tasks", []))
            
            summary = f"""
Task execution summary:
- Total tasks: {total_tasks}
- Successfully completed: {completed_count}
- Failed: {failed_count}
- Success rate: {(completed_count/total_tasks)*100:.1f}% (if total tasks > 0)
- Total duration: {duration}
- Rounds per task: {report.get("subtask_loops", 0)}
"""
        
        return summary
    
    def save_execution_report(self, report: Dict[str, Any]):
        """
        Save execution report in Markdown format
        
        Args:
            report: Execution report
        """
        timestamp = datetime.now().strftime(REPORT_TIMESTAMP_FORMAT)
        markdown_file = os.path.join(self.logs_dir, f"execution_report_{timestamp}.md")
        
        try:
            start_time = datetime.fromisoformat(report["start_time"])
            end_time = datetime.fromisoformat(report["end_time"])
            duration = end_time - start_time
            
            # Check if it's the new todo file processing mode
            if "todo_file" in report:
                markdown_content = self._build_todo_file_report(report, start_time, end_time, duration)
            else:
                markdown_content = self._build_legacy_report(report, start_time, end_time, duration)
            
            # Add system information
            markdown_content += self._build_system_info()
            
            # Save Markdown file
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            print_current(f"ðŸ“ Execution report saved to: {markdown_file}")
            
            # Generate concise summary report (if enabled in config)
            if get_summary_report():
                try:
                    self.generate_summary_report(report, timestamp)
                except Exception as e:
                    print_current(f"âš ï¸ Failed to generate summary report: {e}")
            
        except Exception as e:
            print_current(f"âš ï¸ Failed to save execution report: {e}")
    
    def _build_todo_file_report(self, report: Dict[str, Any], start_time: datetime, 
                               end_time: datetime, duration) -> str:
        """Build report for todo file processing mode"""
        todo_file = report.get("todo_file", "Unknown file")
        success = report.get("success", False)
        task_history = report.get("task_history", [])
        rounds_executed = len([h for h in task_history if isinstance(h, dict) and "round" in h])
        
        markdown_content = f"""# {APP_NAME} Todo File Processing Report

## ðŸ“Š Execution Overview

- **Execution Time**: {start_time.strftime(DATETIME_FORMAT)} - {end_time.strftime(DATETIME_FORMAT)}
- **Total Duration**: {duration}
- **Processed File**: {todo_file}
- **Execution Status**: {'âœ… Success' if success else 'âŒ Failed'}
- **Execution Rounds**: {rounds_executed}/{report.get("subtask_loops", 0)}
- **Workspace Directory**: {report.get("workspace_dir", "Not specified")}

---

"""
        
        # Add execution history details
        if task_history:
            markdown_content += "## ðŸ“‹ Execution Details\n\n"
            markdown_content += self._build_task_history_section(task_history)
        
        # If execution failed, add error information
        if not success and "error" in report:
            markdown_content += f"## âŒ Execution Error\n\n"
            markdown_content += f"**Error Message**: {report['error']}\n\n"
        
        return markdown_content
    
    def _build_legacy_report(self, report: Dict[str, Any], start_time: datetime, 
                            end_time: datetime, duration) -> str:
        """Build report for legacy mode"""
        markdown_content = f"""# {APP_NAME} Task Execution Report

## ðŸ“Š Execution Overview

- **Execution Time**: {start_time.strftime(DATETIME_FORMAT)} - {end_time.strftime(DATETIME_FORMAT)}
- **Total Duration**: {duration}
- **Total Tasks**: {report.get("total_tasks", 0)}
- **Successfully Completed**: {len(report.get("completed_tasks", []))}
- **Failed**: {len(report.get("failed_tasks", []))}
- **Success Rate**: {(len(report.get("completed_tasks", []))/report.get("total_tasks", 1)*100):.1f}%
- **Rounds per Task**: {report.get("subtask_loops", 0)}
- **Workspace Directory**: {report.get("workspace_dir", "Not specified")}

---

"""
        
        # Add details of successfully completed tasks
        completed_tasks = report.get("completed_tasks", [])
        if completed_tasks:
            markdown_content += "## âœ… Successfully Completed Tasks\n\n"
            # ... (maintain original logic for completed tasks)
        
        # Add failed tasks
        failed_tasks = report.get("failed_tasks", [])
        if failed_tasks:
            markdown_content += "## âŒ Failed Tasks\n\n"
            for failed_task in failed_tasks:
                task_id = failed_task.get("task_id", "Unknown")
                task_name = failed_task.get("task_name", "Unknown Task")
                error = failed_task.get("error", "Unknown Error")
                timestamp_str = failed_task.get("timestamp", "")
                
                markdown_content += f"### Task {task_id}: {task_name}\n\n"
                
                if timestamp_str:
                    try:
                        fail_time = datetime.fromisoformat(timestamp_str)
                        markdown_content += f"**Failure Time**: {fail_time.strftime(DATETIME_FORMAT)}\n\n"
                    except:
                        pass
                
                markdown_content += f"**Error Message**: {error}\n\n"
        
        return markdown_content
    
    def _build_task_history_section(self, task_history: List[Dict[str, Any]]) -> str:
        """Build task history section for the report"""
        content = ""
        
        for round_info in task_history:
            if isinstance(round_info, dict) and "round" in round_info:
                round_num = round_info["round"]
                prompt = round_info.get("prompt", "")
                result = round_info.get("result", "")
                task_completed = round_info.get("task_completed", False)
                timestamp_str = round_info.get("timestamp", "")
                
                content += f"### Round {round_num} Execution\n\n"
                
                if timestamp_str:
                    try:
                        exec_time = datetime.fromisoformat(timestamp_str)
                        content += f"**Execution Time**: {exec_time.strftime('%H:%M:%S')}\n\n"
                    except:
                        pass
                
                # Add user prompt (simplified display)
                if prompt:
                    # ä½¿ç”¨é…ç½®çš„åŽ†å²æˆªæ–­é•¿åº¦
                    history_truncation_length = get_truncation_length()
                    content += f"**Task Requirements**: {prompt[:history_truncation_length]}...\n\n"
                
                # Parse result content
                if result:
                    content += self._parse_result_content(result)
                
                # Task completion status
                if task_completed:
                    content += "**Status**: ðŸŽ‰ Task Completed\n\n"
                else:
                    content += "**Status**: ðŸ”„ Continue Execution\n\n"
                
                content += "---\n\n"
            
            elif isinstance(round_info, dict) and "role" in round_info:
                # Skip system message records
                continue
            
            elif isinstance(round_info, dict) and "error" in round_info:
                # Handle error records
                round_num = round_info.get("round", "Unknown")
                error_msg = round_info.get("error", "Unknown error")
                content += f"### âŒ Round {round_num} Execution Error\n\n"
                content += f"**Error Message**: {error_msg}\n\n---\n\n"
        
        return content
    
    def _parse_result_content(self, result: str) -> str:
        """Parse and format result content"""
        content = ""
        
        # Separate LLM response and tool execution results
        if "--- Tool Execution Result ---" in result:
            separator = "--- Tool Execution Result ---"
            parts = result.split(separator)
            llm_response = parts[0].strip()
            tool_results = parts[1].strip() if len(parts) > 1 else ""
            
            # LLM response
            if llm_response:
                # ä½¿ç”¨é…ç½®çš„ä¸»æˆªæ–­é•¿åº¦
                main_truncation_length = get_truncation_length()
                content += f"**LLM Response**:\n```\n{llm_response[:main_truncation_length]}{'...' if len(llm_response) > main_truncation_length else ''}\n```\n\n"
            
            # Tool execution results
            if tool_results:
                content += f"**Tool Execution Results**:\n```\n{tool_results[:main_truncation_length]}{'...' if len(tool_results) > main_truncation_length else ''}\n```\n\n"
        else:
            # Plain text response
            main_truncation_length = get_truncation_length()
            content += f"**Execution Result**:\n```\n{result[:main_truncation_length]}{'...' if len(result) > main_truncation_length else ''}\n```\n\n"
        
        return content
    
    def _build_system_info(self) -> str:
        """Build system information section"""
        return f"""---

## ðŸ”§ System Information

This report is generated by the {APP_NAME} automated task processing system.

- **System Version**: {APP_NAME} v1.0
- **Report Format**: Human-readable Markdown format
- **Generation Time**: {datetime.now().strftime(DATETIME_FORMAT)}

### ðŸ“ Related Files

- Detailed execution logs available in the logs directory

---

*This report contains complete task execution history, tool call results, and detailed execution information for analysis and debugging.*
"""
    
    def generate_summary_report(self, report: Dict[str, Any], timestamp: str):
        """
        Generate task summary report based on language configuration
        
        Args:
            report: Execution report
            timestamp: Timestamp
        """
        lang = read_language_config()
        
        if lang == "en":
            self._generate_summary_report_en(report, timestamp)
        else:
            self._generate_summary_report_zh(report, timestamp)
    
    def _generate_summary_report_zh(self, report: Dict[str, Any], timestamp: str):
        """Generate Chinese summary report using LLM"""
        try:
            summary_file = os.path.join(self.workspace_dir or self.logs_dir, f"task_summary_{timestamp}.md")
            
            analysis_content = self._build_analysis_content_zh(report)
            summary_prompt = self._build_summary_prompt_zh(analysis_content)
            
            print_current("ðŸ§  ä½¿ç”¨å¤§æ¨¡åž‹ç”Ÿæˆä»»åŠ¡æ€»ç»“æŠ¥å‘Š...")
            
            if self.executor.is_claude:
                llm_summary = self._generate_with_claude_zh(summary_prompt)
            else:
                llm_summary = self._generate_with_openai_zh(summary_prompt)
            
            final_summary = self._build_final_summary_zh(report, timestamp, llm_summary)
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(final_summary)
            
            print_current(f"ðŸ“‹ ä»»åŠ¡æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
            
        except Exception as e:
            print_current(f"âš ï¸ ç”Ÿæˆæ€»ç»“æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _generate_summary_report_en(self, report: Dict[str, Any], timestamp: str):
        """Generate English summary report using LLM"""
        try:
            summary_file = os.path.join(self.workspace_dir or self.logs_dir, f"task_summary_{timestamp}.md")
            
            analysis_content = self._build_analysis_content_en(report)
            summary_prompt = self._build_summary_prompt_en(analysis_content)
            
            print_current("ðŸ§  Using large model to generate task summary report...")
            
            if self.executor.is_claude:
                llm_summary = self._generate_with_claude_en(summary_prompt)
            else:
                llm_summary = self._generate_with_openai_en(summary_prompt)
            
            final_summary = self._build_final_summary_en(report, timestamp, llm_summary)
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(final_summary)
            
            print_current(f"ðŸ“‹ Task summary report saved to: {summary_file}")
            
        except Exception as e:
            print_current(f"âš ï¸ Failed to generate summary report: {e}")
    
    # ... (çœç•¥å…¶ä»–ç§æœ‰æ–¹æ³•çš„å…·ä½“å®žçŽ°ï¼Œè¿™äº›æ–¹æ³•ä¸ŽåŽŸæ¥çš„å®žçŽ°ç±»ä¼¼)
    def _build_analysis_content_zh(self, report: Dict[str, Any]) -> str:
        """Build Chinese analysis content"""
        return "è¯·åˆ†æžä»¥ä¸‹ä»»åŠ¡æ‰§è¡Œä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“æŠ¥å‘Šï¼š\n\n"
    
    def _build_analysis_content_en(self, report: Dict[str, Any]) -> str:
        """Build English analysis content"""
        return "Please analyze the following task execution information and generate a concise summary report:\n\n"
    
    def _build_summary_prompt_zh(self, analysis_content: str) -> str:
        """Build Chinese summary prompt"""
        return f"{analysis_content}\n\nè¯·ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“æŠ¥å‘Šã€‚"
    
    def _build_summary_prompt_en(self, analysis_content: str) -> str:
        """Build English summary prompt"""
        return f"{analysis_content}\n\nPlease generate a concise summary report."
    
    def _generate_with_claude_zh(self, prompt: str) -> str:
        """Generate Chinese summary with Claude"""
        return "Chinese summary placeholder"
    
    def _generate_with_openai_zh(self, prompt: str) -> str:
        """Generate Chinese summary with OpenAI"""
        return "Chinese summary placeholder"
    
    def _generate_with_claude_en(self, prompt: str) -> str:
        """Generate English summary with Claude"""
        return "English summary placeholder"
    
    def _generate_with_openai_en(self, prompt: str) -> str:
        """Generate English summary with OpenAI"""
        return "English summary placeholder"
    
    def _build_final_summary_zh(self, report: Dict[str, Any], timestamp: str, llm_summary: str) -> str:
        """Build final Chinese summary"""
        return f"# ä»»åŠ¡æ€»ç»“\n\n{llm_summary}"
    
    def _build_final_summary_en(self, report: Dict[str, Any], timestamp: str, llm_summary: str) -> str:
        """Build final English summary"""
        return f"# Task Summary\n\n{llm_summary}"