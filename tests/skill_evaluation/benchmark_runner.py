#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯„æµ‹å£³ç¨‹åº
å®ç°å®Œæ•´çš„è¯„æµ‹æµç¨‹ï¼šåŸºçº¿æµ‹è¯• -> ä»»åŠ¡æ•´ç† -> Skillæ•´ç† -> Skillæµ‹è¯• -> å¯¹æ¯”åˆ†æ
"""

import os
import sys
import json
import argparse
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from tests.skill_evaluation.test_dataset import TestDataset, TestCase
from tests.skill_evaluation.evaluator import TaskEvaluator
from src.skill_evolve.task_reflection import TaskReflection
from src.skill_evolve.skill_manager import SkillManager
from src.config_loader import get_gui_default_data_directory


class BenchmarkRunner:
    """è¯„æµ‹è¿è¡Œå™¨"""
    
    def __init__(self, 
                 root_dir: Optional[str] = None,
                 config_file: str = "config/config.txt",
                 user_id: Optional[str] = None):
        """
        åˆå§‹åŒ–è¯„æµ‹è¿è¡Œå™¨
        
        Args:
            root_dir: æ ¹ç›®å½•ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä»configè¯»å–ï¼‰
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            user_id: ç”¨æˆ·ID
        """
        self.config_file = config_file
        
        # ç¡®å®šæ ¹ç›®å½•
        if root_dir:
            self.root_dir = os.path.abspath(root_dir)
        else:
            data_dir = get_gui_default_data_directory(config_file)
            if data_dir:
                self.root_dir = data_dir
            else:
                project_root = self._find_project_root()
                self.root_dir = os.path.join(project_root, "data") if project_root else "data"
        
        self.user_id = user_id
        self.test_dataset = TestDataset()
        self.test_dataset.load_test_cases()
        
        # åˆ›å»ºè¯„æµ‹ç»“æœç›®å½•
        self.results_dir = os.path.join(self.root_dir, "benchmark_results")
        os.makedirs(self.results_dir, exist_ok=True)
        
        print(f"ğŸ“ æ ¹ç›®å½•: {self.root_dir}")
        print(f"ğŸ“ ç»“æœç›®å½•: {self.results_dir}")
        print(f"ğŸ“‹ æµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(self.test_dataset.test_cases)}")
    
    def _find_project_root(self) -> Optional[str]:
        """æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•"""
        current = Path(__file__).parent.resolve()
        for _ in range(10):
            config_dir = current / "config"
            if config_dir.exists() and config_dir.is_dir():
                return str(current)
            if current == current.parent:
                break
            current = current.parent
        return None
    
    def run_baseline(self) -> Dict[str, Any]:
        """
        è¿è¡ŒåŸºçº¿æµ‹è¯•ï¼ˆæ— skillï¼‰
        
        Returns:
            åŸºçº¿æµ‹è¯•ç»“æœ
        """
        print("\n" + "="*60)
        print("ç¬¬ä¸€é˜¶æ®µï¼šåŸºçº¿æµ‹è¯•ï¼ˆæ— skillï¼‰")
        print("="*60)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        baseline_dir = os.path.join(self.results_dir, f"baseline_{timestamp}")
        os.makedirs(baseline_dir, exist_ok=True)
        
        results = {
            "timestamp": timestamp,
            "test_type": "baseline",
            "test_cases": [],
            "summary": {}
        }
        
        evaluator = TaskEvaluator(
            root_dir=baseline_dir,
            config_file=self.config_file,
            user_id=self.user_id,
            enable_long_term_memory=False
        )
        
        total_score = 0.0
        success_count = 0
        
        for i, test_case in enumerate(self.test_dataset.test_cases, 1):
            print(f"\n[{i}/{len(self.test_dataset.test_cases)}] æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹: {test_case.task_id}")
            print(f"ä»»åŠ¡æè¿°: {test_case.task_description[:100]}...")
            
            # æ‰§è¡Œä»»åŠ¡
            execution_result = evaluator.execute_task(test_case, "baseline_outputs")
            
            # è¯„ä¼°ç»“æœ
            evaluation = evaluator.calculate_score(test_case, execution_result)
            
            results["test_cases"].append(evaluation)
            
            total_score += evaluation["total_score"]
            if evaluation["success"]:
                success_count += 1
            
            print(f"  å¾—åˆ†: {evaluation['total_score']:.2f} | "
                  f"å®Œæˆåº¦: {evaluation['completion_score']:.2f} | "
                  f"è´¨é‡: {evaluation['quality_score']:.2f} | "
                  f"æ•ˆç‡: {evaluation['efficiency_score']:.2f} | "
                  f"åˆ›æ–°: {evaluation['innovation_score']:.2f}")
            print(f"  æˆåŠŸ: {'æ˜¯' if evaluation['success'] else 'å¦'} | "
                  f"è½®æ•°: {evaluation['rounds']} | "
                  f"å·¥å…·è°ƒç”¨: {evaluation['tool_calls']} | "
                  f"ä½¿ç”¨skill: {'æ˜¯' if evaluation['skill_used'] else 'å¦'}")
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        avg_score = total_score / len(self.test_dataset.test_cases) if self.test_dataset.test_cases else 0.0
        success_rate = success_count / len(self.test_dataset.test_cases) if self.test_dataset.test_cases else 0.0
        
        results["summary"] = {
            "total_cases": len(self.test_dataset.test_cases),
            "success_count": success_count,
            "success_rate": success_rate,
            "average_score": avg_score,
            "total_score": total_score
        }
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(baseline_dir, "results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nåŸºçº¿æµ‹è¯•å®Œæˆï¼")
        print(f"å¹³å‡å¾—åˆ†: {avg_score:.2f}")
        print(f"æˆåŠŸç‡: {success_rate:.2%}")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        return results
    
    def run_task_reflection(self) -> bool:
        """
        è¿è¡Œä»»åŠ¡æ•´ç†ï¼Œç”Ÿæˆskill
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print("\n" + "="*60)
        print("ç¬¬äºŒé˜¶æ®µï¼šä»»åŠ¡æ•´ç†ï¼ˆç”Ÿæˆskillï¼‰")
        print("="*60)
        
        try:
            task_reflection = TaskReflection(
                root_dir=self.root_dir,
                config_file=self.config_file
            )
            
            print("å¼€å§‹å¤„ç†ä»»åŠ¡æ—¥å¿—ï¼Œç”Ÿæˆskill...")
            task_reflection.run()
            
            print("ä»»åŠ¡æ•´ç†å®Œæˆï¼")
            return True
            
        except Exception as e:
            print(f"âŒ ä»»åŠ¡æ•´ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_skill_manager(self) -> bool:
        """
        è¿è¡Œskillæ•´ç†ï¼Œæ•´åˆskill
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print("\n" + "="*60)
        print("ç¬¬ä¸‰é˜¶æ®µï¼šSkillæ•´ç†ï¼ˆæ•´åˆskillï¼‰")
        print("="*60)
        
        try:
            skill_manager = SkillManager(
                root_dir=self.root_dir,
                config_file=self.config_file
            )
            
            print("å¼€å§‹æ•´ç†skillï¼Œè¿›è¡Œåˆå¹¶å’Œæ•´åˆ...")
            skill_manager.run()
            
            print("Skillæ•´ç†å®Œæˆï¼")
            return True
            
        except Exception as e:
            print(f"âŒ Skillæ•´ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_with_skills(self) -> Dict[str, Any]:
        """
        è¿è¡Œskillæµ‹è¯•ï¼ˆæœ‰skillï¼‰
        
        Returns:
            Skillæµ‹è¯•ç»“æœ
        """
        print("\n" + "="*60)
        print("ç¬¬å››é˜¶æ®µï¼šSkillæµ‹è¯•ï¼ˆæœ‰skillï¼‰")
        print("="*60)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        skill_dir = os.path.join(self.results_dir, f"skill_{timestamp}")
        os.makedirs(skill_dir, exist_ok=True)
        
        results = {
            "timestamp": timestamp,
            "test_type": "skill",
            "test_cases": [],
            "summary": {}
        }
        
        # æ³¨æ„ï¼šskillå·¥å…·ä¼šåœ¨long_term_memoryå¯ç”¨æ—¶è‡ªåŠ¨æ³¨å†Œ
        # éœ€è¦åœ¨config.txtä¸­è®¾ç½®enable_long_term_memory=True
        # æˆ–è€…ç¡®ä¿ç¯å¢ƒå˜é‡AGIBOT_LONG_TERM_MEMORYä¸ä¸º'false'/'0'/'no'/'off'
        # æ³¨æ„ï¼šTaskEvaluatorä¸­çš„enable_long_term_memoryå‚æ•°ç›®å‰ä»…ç”¨äºæ ‡è®°ï¼Œå®é™…å¯ç”¨éœ€è¦é€šè¿‡config.txt
        evaluator = TaskEvaluator(
            root_dir=skill_dir,
            config_file=self.config_file,
            user_id=self.user_id,
            enable_long_term_memory=True
        )
        
        total_score = 0.0
        success_count = 0
        
        for i, test_case in enumerate(self.test_dataset.test_cases, 1):
            print(f"\n[{i}/{len(self.test_dataset.test_cases)}] æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹: {test_case.task_id}")
            print(f"ä»»åŠ¡æè¿°: {test_case.task_description[:100]}...")
            
            # æ‰§è¡Œä»»åŠ¡
            execution_result = evaluator.execute_task(test_case, "skill_outputs")
            
            # è¯„ä¼°ç»“æœ
            evaluation = evaluator.calculate_score(test_case, execution_result)
            
            results["test_cases"].append(evaluation)
            
            total_score += evaluation["total_score"]
            if evaluation["success"]:
                success_count += 1
            
            print(f"  å¾—åˆ†: {evaluation['total_score']:.2f} | "
                  f"å®Œæˆåº¦: {evaluation['completion_score']:.2f} | "
                  f"è´¨é‡: {evaluation['quality_score']:.2f} | "
                  f"æ•ˆç‡: {evaluation['efficiency_score']:.2f} | "
                  f"åˆ›æ–°: {evaluation['innovation_score']:.2f}")
            print(f"  æˆåŠŸ: {'æ˜¯' if evaluation['success'] else 'å¦'} | "
                  f"è½®æ•°: {evaluation['rounds']} | "
                  f"å·¥å…·è°ƒç”¨: {evaluation['tool_calls']} | "
                  f"ä½¿ç”¨skill: {'æ˜¯' if evaluation['skill_used'] else 'å¦'}")
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        avg_score = total_score / len(self.test_dataset.test_cases) if self.test_dataset.test_cases else 0.0
        success_rate = success_count / len(self.test_dataset.test_cases) if self.test_dataset.test_cases else 0.0
        
        results["summary"] = {
            "total_cases": len(self.test_dataset.test_cases),
            "success_count": success_count,
            "success_rate": success_rate,
            "average_score": avg_score,
            "total_score": total_score
        }
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(skill_dir, "results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nSkillæµ‹è¯•å®Œæˆï¼")
        print(f"å¹³å‡å¾—åˆ†: {avg_score:.2f}")
        print(f"æˆåŠŸç‡: {success_rate:.2%}")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        return results
    
    def compare_results(self, baseline_results: Dict[str, Any], skill_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¯¹æ¯”ä¸¤æ¬¡æµ‹è¯•ç»“æœ
        
        Args:
            baseline_results: åŸºçº¿æµ‹è¯•ç»“æœ
            skill_results: Skillæµ‹è¯•ç»“æœ
            
        Returns:
            å¯¹æ¯”ç»“æœ
        """
        print("\n" + "="*60)
        print("ç¬¬äº”é˜¶æ®µï¼šç»“æœå¯¹æ¯”åˆ†æ")
        print("="*60)
        
        comparison = {
            "baseline": baseline_results["summary"],
            "skill": skill_results["summary"],
            "improvements": {},
            "detailed_comparison": []
        }
        
        # è®¡ç®—æ”¹è¿›æŒ‡æ ‡
        baseline_avg = baseline_results["summary"]["average_score"]
        skill_avg = skill_results["summary"]["average_score"]
        score_improvement = skill_avg - baseline_avg
        score_improvement_pct = (score_improvement / baseline_avg * 100) if baseline_avg > 0 else 0
        
        baseline_success_rate = baseline_results["summary"]["success_rate"]
        skill_success_rate = skill_results["summary"]["success_rate"]
        success_rate_improvement = skill_success_rate - baseline_success_rate
        success_rate_improvement_pct = (success_rate_improvement / baseline_success_rate * 100) if baseline_success_rate > 0 else 0
        
        comparison["improvements"] = {
            "score_improvement": score_improvement,
            "score_improvement_pct": score_improvement_pct,
            "success_rate_improvement": success_rate_improvement,
            "success_rate_improvement_pct": success_rate_improvement_pct
        }
        
        # è¯¦ç»†å¯¹æ¯”æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹
        for i, (baseline_case, skill_case) in enumerate(zip(
            baseline_results["test_cases"],
            skill_results["test_cases"]
        )):
            case_comparison = {
                "task_id": baseline_case["task_id"],
                "baseline_score": baseline_case["total_score"],
                "skill_score": skill_case["total_score"],
                "score_improvement": skill_case["total_score"] - baseline_case["total_score"],
                "baseline_success": baseline_case["success"],
                "skill_success": skill_case["success"],
                "baseline_rounds": baseline_case["rounds"],
                "skill_rounds": skill_case["rounds"],
                "rounds_improvement": baseline_case["rounds"] - skill_case["rounds"],
                "baseline_tool_calls": baseline_case["tool_calls"],
                "skill_tool_calls": skill_case["tool_calls"],
                "tool_calls_improvement": baseline_case["tool_calls"] - skill_case["tool_calls"],
                "skill_used": skill_case["skill_used"]
            }
            comparison["detailed_comparison"].append(case_comparison)
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        print(f"\nğŸ“Š æ€»ä½“å¯¹æ¯”:")
        print(f"  å¹³å‡å¾—åˆ†: {baseline_avg:.2f} -> {skill_avg:.2f} "
              f"({'+' if score_improvement >= 0 else ''}{score_improvement:.2f}, "
              f"{'+' if score_improvement_pct >= 0 else ''}{score_improvement_pct:.1f}%)")
        print(f"  æˆåŠŸç‡: {baseline_success_rate:.2%} -> {skill_success_rate:.2%} "
              f"({'+' if success_rate_improvement >= 0 else ''}{success_rate_improvement:.2%}, "
              f"{'+' if success_rate_improvement_pct >= 0 else ''}{success_rate_improvement_pct:.1f}%)")
        
        print(f"\nğŸ“‹ è¯¦ç»†å¯¹æ¯”:")
        for case_comp in comparison["detailed_comparison"]:
            print(f"\n  æµ‹è¯•ç”¨ä¾‹: {case_comp['task_id']}")
            print(f"    å¾—åˆ†: {case_comp['baseline_score']:.2f} -> {case_comp['skill_score']:.2f} "
                  f"({'+' if case_comp['score_improvement'] >= 0 else ''}{case_comp['score_improvement']:.2f})")
            print(f"    æˆåŠŸ: {case_comp['baseline_success']} -> {case_comp['skill_success']}")
            print(f"    è½®æ•°: {case_comp['baseline_rounds']} -> {case_comp['skill_rounds']} "
                  f"({'+' if case_comp['rounds_improvement'] >= 0 else ''}{case_comp['rounds_improvement']})")
            print(f"    å·¥å…·è°ƒç”¨: {case_comp['baseline_tool_calls']} -> {case_comp['skill_tool_calls']} "
                  f"({'+' if case_comp['tool_calls_improvement'] >= 0 else ''}{case_comp['tool_calls_improvement']})")
            print(f"    ä½¿ç”¨skill: {case_comp['skill_used']}")
        
        return comparison
    
    def generate_report(self, comparison: Dict[str, Any], output_file: Optional[str] = None) -> str:
        """
        ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š
        
        Args:
            comparison: å¯¹æ¯”ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
            
        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = os.path.join(self.results_dir, f"report_{timestamp}.json")
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        return output_file
    
    def run_full_benchmark(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´è¯„æµ‹æµç¨‹
        
        Returns:
            å®Œæ•´çš„è¯„æµ‹ç»“æœ
        """
        print("\n" + "="*60)
        print("å¼€å§‹å®Œæ•´è¯„æµ‹æµç¨‹")
        print("="*60)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šåŸºçº¿æµ‹è¯•
        baseline_results = self.run_baseline()
        
        # ç¬¬äºŒé˜¶æ®µï¼šä»»åŠ¡æ•´ç†
        if not self.run_task_reflection():
            print("âš ï¸ è­¦å‘Šï¼šä»»åŠ¡æ•´ç†å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šSkillæ•´ç†
        if not self.run_skill_manager():
            print("âš ï¸ è­¦å‘Šï¼šSkillæ•´ç†å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤")
        
        # ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿skillå·²ç»ä¿å­˜
        time.sleep(2)
        
        # ç¬¬å››é˜¶æ®µï¼šSkillæµ‹è¯•
        skill_results = self.run_with_skills()
        
        # ç¬¬äº”é˜¶æ®µï¼šç»“æœå¯¹æ¯”
        comparison = self.compare_results(baseline_results, skill_results)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_report(comparison)
        
        print("\n" + "="*60)
        print("å®Œæ•´è¯„æµ‹æµç¨‹ç»“æŸ")
        print("="*60)
        
        return {
            "baseline": baseline_results,
            "skill": skill_results,
            "comparison": comparison,
            "report_file": report_file
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Skillç³»ç»Ÿè¯„æµ‹ç¨‹åº")
    parser.add_argument("--root-dir", type=str, help="æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--config", type=str, default="config/config.txt", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--user-id", type=str, help="ç”¨æˆ·ID")
    parser.add_argument("--baseline-only", action="store_true", help="åªè¿è¡ŒåŸºçº¿æµ‹è¯•")
    parser.add_argument("--skill-only", action="store_true", help="åªè¿è¡Œskillæµ‹è¯•")
    parser.add_argument("--reflection-only", action="store_true", help="åªè¿è¡Œä»»åŠ¡æ•´ç†")
    parser.add_argument("--manager-only", action="store_true", help="åªè¿è¡Œskillæ•´ç†")
    
    args = parser.parse_args()
    
    runner = BenchmarkRunner(
        root_dir=args.root_dir,
        config_file=args.config,
        user_id=args.user_id
    )
    
    if args.baseline_only:
        runner.run_baseline()
    elif args.skill_only:
        runner.run_with_skills()
    elif args.reflection_only:
        runner.run_task_reflection()
    elif args.manager_only:
        runner.run_skill_manager()
    else:
        runner.run_full_benchmark()


if __name__ == "__main__":
    main()

